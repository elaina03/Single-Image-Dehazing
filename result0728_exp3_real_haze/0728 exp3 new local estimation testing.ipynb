{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os \n",
    "import sys\n",
    "import random\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import torchvision.utils as vutils    \n",
    "\n",
    "from new_model_final import bn_init_as_tf, weights_init_xavier,encoder, decoder_T, decoder_A, refinement_final\n",
    "from data_final import UnNormalize#,getTestLoader_v2,getValLoader, UnNormalize\n",
    "from utils import AverageMeter\n",
    "# from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.measure import compare_psnr, compare_ssim\n",
    "import cv2\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "from skimage.util.shape import view_as_blocks\n",
    "from os import listdir\n",
    "from PIL import Image,ImageEnhance\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG','.bmp'])\n",
    "def _is_pil_image(img):\n",
    "    return isinstance(img, Image.Image)\n",
    "def _is_numpy_image(img):\n",
    "    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./exp3_epoch92.pth\n",
      "./real_haze\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "class Param:\n",
    "    def __init__(self):\n",
    "        self.test_dir = './real_haze'\n",
    "        self.checkpoint_path = './exp3_epoch92.pth'\n",
    "        self.image_size=320\n",
    "        self.blocks=3\n",
    "        self.retrain=False\n",
    "        \n",
    "opt= Param()\n",
    "print(opt.checkpoint_path)\n",
    "print(opt.test_dir)\n",
    "print(opt.blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcd(x,y):\n",
    "    while(y):\n",
    "        x , y = y , x % y\n",
    "    return x\n",
    "\n",
    "def lcm(x,y):\n",
    "    \n",
    "    result = (x*y)//gcd(x,y)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original testset + overlap\n",
    "class CreateTestDataSet(Dataset):\n",
    "    def __init__(self, test_dir, block_num = 3,transform=None, overlap=False):\n",
    "        self.dir= test_dir\n",
    "        self.transform=transform\n",
    "        self.data_files = {'haze':[]}\n",
    "        self.img_ratio= lcm(32,block_num)\n",
    "        self.block_num = block_num\n",
    "        self.overlap=overlap\n",
    "#         self.data_files = {'haze':[],'GT':[]}\n",
    "\n",
    "        for key in self.data_files.keys():\n",
    "            subdir = join(self.dir, key)\n",
    "            self.data_files[key] += [join(subdir,x) for x in listdir(subdir) if is_image_file(x) ]\n",
    "            # self.data_files[key].sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        haze_name = self.data_files['haze'][index]\n",
    "        haze = Image.open(haze_name).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            # apply transform to each sample in data_files\n",
    "            haze = self.transform(haze)\n",
    "            \n",
    "        c,h,w = haze.shape\n",
    "        new_h, new_w = self.img_ratio*math.ceil(h/self.img_ratio), self.img_ratio*math.ceil(w/self.img_ratio)\n",
    "        h_block,w_block = (new_h//self.block_num), (new_w//self.block_num)\n",
    "        if self.overlap:\n",
    "            new_h, new_w = new_h+h_block, new_w+w_block\n",
    "        \n",
    "        h_pad1, h_pad2 = 0,0\n",
    "        w_pad1, w_pad2= 0,0\n",
    "        if new_h != h:\n",
    "            pad= new_h-h\n",
    "            h_pad1, h_pad2 = pad//2, (pad-pad//2)\n",
    "        if new_w != w:\n",
    "            pad= new_w-w\n",
    "            w_pad1, w_pad2 = pad//2, (pad-pad//2)\n",
    "            \n",
    "        haze_pad = F.pad(haze.unsqueeze(0), (w_pad1, w_pad2, h_pad1, h_pad2), mode='reflect').squeeze(0)\n",
    "        all_blocks = view_as_blocks(haze_pad.numpy(), block_shape=(c,h_block,w_block)).reshape(-1,c,h_block,w_block)\n",
    "#         print(\"all blocks size\", all_blocks.shape)\n",
    "        # all_blocks: (total_block_num, channel, h, w)\n",
    "        all_blocks = torch.from_numpy(all_blocks)\n",
    "        \n",
    "        info = (h,w,h_pad1,w_pad1)\n",
    "        block_info = (h_block, w_block)      \n",
    "        sample={'haze': all_blocks, 'info': info, 'block_info': block_info}       \n",
    "        return sample\n",
    "    def __len__(self):\n",
    "        return len(self.data_files['haze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestLoader(test_dir, blocks=3, overlap=False):\n",
    "    # val_test_transform = transforms.Resize((image_size,image_size))\n",
    "    test_transform = transforms.Compose([\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))\n",
    "                                             # Scale((image_size, image_size), use_trans_atmos=trans_atmos),\n",
    "#                                              ToTensor(use_trans_atmos = trans_atmos),\n",
    "#                                              Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],use_trans_atmos = trans_atmos)\n",
    "                                        ])\n",
    "    test_set = CreateTestDataSet(test_dir, blocks,test_transform, overlap)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "    return test_loader, test_set.__len__()\n",
    "\n",
    "def getTestLoader_OverLap(test_dir, blocks=3, overlap=True):\n",
    "    # val_test_transform = transforms.Resize((image_size,image_size))\n",
    "    test_transform = transforms.Compose([\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))\n",
    "                                             # Scale((image_size, image_size), use_trans_atmos=trans_atmos),\n",
    "#                                              ToTensor(use_trans_atmos = trans_atmos),\n",
    "#                                              Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],use_trans_atmos = trans_atmos)\n",
    "                                        ])\n",
    "    test_set = CreateTestDataSet(test_dir, blocks,test_transform, overlap)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "    return test_loader, test_set.__len__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_0_1(x):\n",
    "    \"\"\"\n",
    "    normalize input tensor x to [0,1]\n",
    "    x: tensor, (B,C,H,W)\n",
    "    \"\"\"\n",
    "    b,c,h,w = x.shape\n",
    "    x = x.view(b,-1)\n",
    "    x -= x.min(1, keepdim=True)[0]\n",
    "    x /= x.max(1, keepdim=True)[0]\n",
    "    if torch.any(torch.isnan(x)):\n",
    "        # avoid NaNs caused by dividing 0, should not happen\n",
    "        x[torch.isnan(x)]=1.0\n",
    "        print(\"divide by 0\")\n",
    "    x = x.view(b,c,h,w)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Test, self).__init__()\n",
    "        self.encoder = encoder()\n",
    "        self.decoder_T = decoder_T(1,self.encoder.feat_out_channels)\n",
    "        self.decoder_A = decoder_A(3,self.encoder.feat_out_channels, return_value=True)\n",
    "        self.generate_dehaze = refinement_final()\n",
    "        self.unnormalize_fun = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "\n",
    "    def forward(self, x):\n",
    "        resized_x = F.interpolate(x, size = (320,320), mode='bilinear',align_corners=True)\n",
    "        skip_feat = self.encoder(x)\n",
    "        trans_map = self.decoder_T(skip_feat)\n",
    "        print(\"In Model Test: trans_map shape:\", trans_map.shape)\n",
    "        trans_map = trans_map.repeat(1,3,1,1)\n",
    "        \n",
    "        atmos_skip_feat = self.encoder(resized_x)\n",
    "        atmos_light = self.decoder_A(atmos_skip_feat)\n",
    "        atmos_light = atmos_light.unsqueeze(-1).unsqueeze(-1)\n",
    "        atmos_light = atmos_light.repeat(1,1,x.shape[-2],x.shape[-1])\n",
    "        \n",
    "        # Unnormalize haze images\n",
    "        hazes_unnormalized = self.unnormalize_fun(x)\n",
    "        # Reconstruct clean images\n",
    "        nonhaze_rec = (hazes_unnormalized-atmos_light*(1-trans_map))\n",
    "        nonhaze_rec = nonhaze_rec/trans_map\n",
    "        nonhaze_rec = torch.clamp(nonhaze_rec, 0.0, 1.0)\n",
    "            \n",
    "        # Refinement Module\n",
    "        nonhaze_refinement = self.generate_dehaze(nonhaze_rec, hazes_unnormalized, trans_map, atmos_light)\n",
    "        \n",
    "        return trans_map, atmos_light, nonhaze_rec, nonhaze_refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aerial', 'castle', 'cityscape', 'cliff', 'forest', 'highquality13', 'img33', 'img54', 'img69', 'landscape', 'lviv', 'manhattan1', 'manhattan2', 'redbrickshouse', 'road', 'swans', 'yosemite1']\n"
     ]
    }
   ],
   "source": [
    "img_list=['aerial','castle','cityscape','cliff','forest','highquality13','img33','img54','img69','landscape','lviv','manhattan1','manhattan2','redbrickshouse','road','swans','yosemite1']\n",
    "# img_list=['cityscape','landscape']\n",
    "print(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./exp3_epoch92.pth\n",
      "./real_haze1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(opt.checkpoint_path)\n",
    "print(opt.test_dir)\n",
    "print(opt.blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(opt.checkpoint_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Test()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dehaze checkpoint './exp3_epoch92.pth'\n",
      "Loading dehaze checkpoint path: ./exp3_epoch92.pth\n",
      "Continuing training at epoch 93\n"
     ]
    }
   ],
   "source": [
    "# Loading dehaze model ( encoder + decoder_D + decoder_A)\n",
    "if opt.checkpoint_path != '' and os.path.isfile(opt.checkpoint_path):\n",
    "    print(\"Loading dehaze checkpoint '{}'\".format(opt.checkpoint_path))\n",
    "    checkpoint = torch.load(opt.checkpoint_path)\n",
    "\n",
    "    start_epoch = checkpoint['epoch']+1\n",
    "    iteration = checkpoint['iteration']+1\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print('Loading dehaze checkpoint path:', opt.checkpoint_path)\n",
    "    # Retrain the loading model\n",
    "    if opt.retrain:\n",
    "        start_epoch =1\n",
    "        iteration=0\n",
    "        print('Restart Training from epoch %d!' % start_epoch)\n",
    "    else:\n",
    "        print('Continuing training at epoch %d' % start_epoch)\n",
    "else:\n",
    "    start_epoch = 1\n",
    "    iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_Test(\n",
       "  (encoder): encoder(\n",
       "    (base_model): Sequential(\n",
       "      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu0): ReLU(inplace=True)\n",
       "      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (denseblock1): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition1): _Transition(\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock2): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition2): _Transition(\n",
       "        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock3): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer17): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer18): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer19): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer20): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer21): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer22): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer23): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer24): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition3): _Transition(\n",
       "        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock4): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder_T): decoder_T(\n",
       "    (dense_block1): BottleneckBlock(\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (trans_block1): TransitionBlock(\n",
       "      (bn1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): ConvTranspose2d(1280, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (residual_block11): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (residual_block12): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (dense_block2): BottleneckBlock(\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(896, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (trans_block2): TransitionBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): ConvTranspose2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (residual_block21): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (residual_block22): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (dense_block3): BottleneckBlock(\n",
       "      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(448, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (trans_block3): TransitionBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): ConvTranspose2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (residual_block31): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (residual_block32): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (dense_block4): BottleneckBlock(\n",
       "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(224, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (trans_block4): TransitionBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): ConvTranspose2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (residual_block41): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (residual_block42): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (dense_block5): BottleneckBlock(\n",
       "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(224, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (trans_block5): TransitionBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): ConvTranspose2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (residual_block51): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (residual_block52): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv1010): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv1020): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv1030): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv1040): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (refine1): Sequential(\n",
       "      (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "    (refine2): Sequential(\n",
       "      (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (refine3): Sequential(\n",
       "      (0): Conv2d(20, 20, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (get_transmission_map): Sequential(\n",
       "      (0): Conv2d(20, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (decoder_A): decoder_A(\n",
       "    (trans_block1): TransitionLayer(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (trans_block2): TransitionLayer(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (trans_block3): TransitionLayer(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (trans_block4): TransitionLayer(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (trans_block5): TransitionLayer(\n",
       "      (bn1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(1536, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fc1): Linear(in_features=3200, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (get_atmosphere_map): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=3, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (generate_dehaze): refinement_final(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(6, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (bn1): BatchNorm2d(64, eps=1.1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (dense_block1): BottleneckBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (residual_block11): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (residual_block12): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (dense_block2): BottleneckBlock(\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(48, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (residual_block21): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (residual_block22): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (get_dehaze_img): Sequential(\n",
       "      (0): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full pass 3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block num: 3\n",
      "Testing Images Number: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"block num:\",opt.blocks)\n",
    "test_loader, test_num = getTestLoader( opt.test_dir,opt.blocks)\n",
    "print(\"Testing Images Number:\", test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: aerial\n",
      "haze shape: torch.Size([9, 3, 160, 224])\n",
      "haze info: 442 622 19 25\n",
      "block info: 160 224\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 160, 224])\n",
      "trans_map shape: torch.Size([9, 3, 160, 224])\n",
      "\n",
      "Image: castle\n",
      "haze shape: torch.Size([9, 3, 224, 224])\n",
      "haze info: 611 619 30 26\n",
      "block info: 224 224\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 224, 224])\n",
      "trans_map shape: torch.Size([9, 3, 224, 224])\n",
      "\n",
      "Image: cityscape\n",
      "haze shape: torch.Size([9, 3, 224, 160])\n",
      "haze info: 600 400 36 40\n",
      "block info: 224 160\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 224, 160])\n",
      "trans_map shape: torch.Size([9, 3, 224, 160])\n",
      "\n",
      "Image: cliff\n",
      "haze shape: torch.Size([9, 3, 128, 192])\n",
      "haze info: 384 576 0 0\n",
      "block info: 128 192\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 128, 192])\n",
      "trans_map shape: torch.Size([9, 3, 128, 192])\n",
      "\n",
      "Image: forest\n",
      "haze shape: torch.Size([9, 3, 256, 352])\n",
      "haze info: 768 1024 0 16\n",
      "block info: 256 352\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 256, 352])\n",
      "trans_map shape: torch.Size([9, 3, 256, 352])\n",
      "\n",
      "Image: highquality13\n",
      "haze shape: torch.Size([9, 3, 192, 256])\n",
      "haze info: 576 768 0 0\n",
      "block info: 192 256\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 192, 256])\n",
      "trans_map shape: torch.Size([9, 3, 192, 256])\n",
      "\n",
      "Image: img33\n",
      "haze shape: torch.Size([9, 3, 256, 192])\n",
      "haze info: 768 576 0 0\n",
      "block info: 256 192\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 256, 192])\n",
      "trans_map shape: torch.Size([9, 3, 256, 192])\n",
      "\n",
      "Image: img54\n",
      "haze shape: torch.Size([9, 3, 256, 416])\n",
      "haze info: 763 1180 2 34\n",
      "block info: 256 416\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 256, 416])\n",
      "trans_map shape: torch.Size([9, 3, 256, 416])\n",
      "\n",
      "Image: img69\n",
      "haze shape: torch.Size([9, 3, 448, 672])\n",
      "haze info: 1328 1999 8 8\n",
      "block info: 448 672\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 448, 672])\n",
      "trans_map shape: torch.Size([9, 3, 448, 672])\n",
      "\n",
      "Image: landscape\n",
      "haze shape: torch.Size([9, 3, 192, 224])\n",
      "haze info: 525 600 25 36\n",
      "block info: 192 224\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 192, 224])\n",
      "trans_map shape: torch.Size([9, 3, 192, 224])\n",
      "\n",
      "Image: lviv\n",
      "haze shape: torch.Size([9, 3, 352, 480])\n",
      "haze info: 1044 1428 6 6\n",
      "block info: 352 480\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 352, 480])\n",
      "trans_map shape: torch.Size([9, 3, 352, 480])\n",
      "\n",
      "Image: manhattan1\n",
      "haze shape: torch.Size([9, 3, 256, 192])\n",
      "haze info: 768 576 0 0\n",
      "block info: 256 192\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 256, 192])\n",
      "trans_map shape: torch.Size([9, 3, 256, 192])\n",
      "\n",
      "Image: manhattan2\n",
      "haze shape: torch.Size([9, 3, 256, 352])\n",
      "haze info: 768 1024 0 16\n",
      "block info: 256 352\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 256, 352])\n",
      "trans_map shape: torch.Size([9, 3, 256, 352])\n",
      "\n",
      "Image: redbrickshouse\n",
      "haze shape: torch.Size([9, 3, 160, 160])\n",
      "haze info: 450 441 15 19\n",
      "block info: 160 160\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 160, 160])\n",
      "trans_map shape: torch.Size([9, 3, 160, 160])\n",
      "\n",
      "Image: road\n",
      "haze shape: torch.Size([9, 3, 160, 224])\n",
      "haze info: 400 600 40 36\n",
      "block info: 160 224\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 160, 224])\n",
      "trans_map shape: torch.Size([9, 3, 160, 224])\n",
      "\n",
      "Image: swans\n",
      "haze shape: torch.Size([9, 3, 160, 224])\n",
      "haze info: 416 624 32 24\n",
      "block info: 160 224\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 160, 224])\n",
      "trans_map shape: torch.Size([9, 3, 160, 224])\n",
      "\n",
      "Image: yosemite1\n",
      "haze shape: torch.Size([9, 3, 256, 192])\n",
      "haze info: 768 576 0 0\n",
      "block info: 256 192\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([9, 1, 256, 192])\n",
      "trans_map shape: torch.Size([9, 3, 256, 192])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "img_dir= join('./result0728_exp3_real_haze','full_pass3x3')\n",
    "block_num= opt.blocks\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "        for i, test_batched in enumerate(test_loader,0):\n",
    "            \n",
    "            print(\"Image:\",img_list[i])\n",
    "            # Prepare sample and target\n",
    "            # haze: (b,c,h,w) with (r,g,b) order\n",
    "            haze = test_batched['haze'][0]\n",
    "            h_origin,w_origin,h_pad,w_pad = test_batched['info']\n",
    "            h_origin,w_origin,h_pad,w_pad = h_origin.item(),w_origin.item(),h_pad.item(),w_pad.item()\n",
    "            h_block, w_block = test_batched['block_info']\n",
    "            h_block, w_block = h_block.item(), w_block.item()\n",
    "            print(\"haze shape:\", haze.shape)\n",
    "            print(\"haze info:\",h_origin,w_origin,h_pad,w_pad)\n",
    "            print(\"block info:\",h_block, w_block)\n",
    "            print()\n",
    "            \n",
    "            total_block_num,c,h,w = haze.shape\n",
    "            \n",
    "            trans_map, atmos_light,nonhaze_rec, dehaze = model(haze.to(device))\n",
    "            print(\"trans_map shape:\", trans_map.shape)\n",
    "                \n",
    "            trans_map_np = np.zeros(shape=(h*block_num,w*block_num,3))\n",
    "            atmos_map_np = np.zeros(shape=(h*block_num,w*block_num,3))\n",
    "            nonhaze_rec_np = np.zeros(shape=(h*block_num,w*block_num,3))\n",
    "            dehaze_np = np.zeros(shape=(h*block_num,w*block_num,3))\n",
    "            start_index=0\n",
    "            \n",
    "            for h_idx in range(block_num):\n",
    "                for w_idx in range(block_num):\n",
    "                    block = trans_map[start_index].cpu().numpy()\n",
    "                    block = np.transpose(block, (1,2,0))\n",
    "                    trans_map_np[h_idx*h_block:(h_idx+1)*h_block,w_idx*w_block:(w_idx+1)*w_block,:] = block\n",
    "                    \n",
    "                    block = atmos_light[start_index].cpu().numpy()\n",
    "                    block = np.transpose(block, (1,2,0))\n",
    "                    atmos_map_np[h_idx*h_block:(h_idx+1)*h_block,w_idx*w_block:(w_idx+1)*w_block,:] = block\n",
    "                    \n",
    "                    block = nonhaze_rec[start_index].cpu().numpy()\n",
    "                    block = np.transpose(block, (1,2,0))\n",
    "                    nonhaze_rec_np[h_idx*h_block:(h_idx+1)*h_block,w_idx*w_block:(w_idx+1)*w_block,:] = block\n",
    "\n",
    "                    block = dehaze[start_index].cpu().numpy()\n",
    "                    block = np.transpose(block, (1,2,0))\n",
    "                    dehaze_np[h_idx*h_block:(h_idx+1)*h_block,w_idx*w_block:(w_idx+1)*w_block,:] = block\n",
    "                    \n",
    "                    start_index += 1\n",
    "\n",
    "    \n",
    "            trans_map_np = trans_map_np[h_pad:h_pad+h_origin,w_pad:w_pad+w_origin,:]\n",
    "            atmos_map_np = atmos_map_np[h_pad:h_pad+h_origin,w_pad:w_pad+w_origin,:]\n",
    "            nonhaze_rec_np = nonhaze_rec_np[h_pad:h_pad+h_origin,w_pad:w_pad+w_origin,:]\n",
    "            dehaze_np = dehaze_np[h_pad:h_pad+h_origin,w_pad:w_pad+w_origin,:]\n",
    "            \n",
    "            trans_map_np = (255*trans_map_np).astype(np.uint8)[:,:,::-1]\n",
    "            atmos_map_np = (255*atmos_map_np).astype(np.uint8)[:,:,::-1]\n",
    "            nonhaze_rec_np = (255*nonhaze_rec_np).astype(np.uint8)[:,:,::-1]\n",
    "            dehaze_np = (255*dehaze_np).astype(np.uint8)[:,:,::-1]\n",
    "            \n",
    "#             print(\"trans dtype:\", trans_map_np.dtype)\n",
    "#             print(\"atmos dtype:\", atmos_map_np.dtype)\n",
    "#             print(\"nonohaze rec dtype:\", nonhaze_rec_np.dtype)\n",
    "#             print(\"dehaze dtype:\", dehaze_np.dtype)\n",
    "            \n",
    "#             cv2.imshow('Trans', trans_map_np)\n",
    "#             cv2.imshow('Atmos', atmos_map_np)\n",
    "#             cv2.imshow('Nonhaze', nonhaze_rec_np)\n",
    "#             cv2.imshow('Dehaze', dehaze_np)\n",
    "#             cv2.waitKey(0)\n",
    "#             cv2.destroyAllWindows()\n",
    "            \n",
    "            atmos_intensity_np = cv2.cvtColor(atmos_map_np, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "            # because opencv is BGR order, we need to change RGB to BGR\n",
    "            cv2.imwrite(join(img_dir,'dehaze',img_list[i]+'_dehaze.png'),dehaze_np)\n",
    "            cv2.imwrite(join(img_dir,'nonhaze_rec',img_list[i]+'_nonhaze_rec.png'),nonhaze_rec_np)\n",
    "            cv2.imwrite(join(img_dir,'trans',img_list[i]+'_trans.png'),trans_map_np)\n",
    "            cv2.imwrite(join(img_dir,'atmos',img_list[i]+'_atmos.png'),atmos_map_np)\n",
    "            cv2.imwrite(join(img_dir,'atmos_intensity',img_list[i] +'_atmos_intensity.png'),atmos_intensity_np)\n",
    "            print()\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full pass 1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./exp3_epoch92.pth\n",
      "./real_haze\n",
      "block number: 1\n"
     ]
    }
   ],
   "source": [
    "opt.blocks=1\n",
    "print(opt.checkpoint_path)\n",
    "print(opt.test_dir)\n",
    "print(\"block number:\",opt.blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Images Number: 17\n"
     ]
    }
   ],
   "source": [
    "test_loader, test_num = getTestLoader( opt.test_dir,opt.blocks)\n",
    "print(\"Testing Images Number:\", test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: aerial\n",
      "haze shape: torch.Size([1, 3, 448, 640])\n",
      "haze info: 442 622 3 9\n",
      "block info: 448 640\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 448, 640])\n",
      "trans_map shape: torch.Size([1, 3, 448, 640])\n",
      "\n",
      "Image: castle\n",
      "haze shape: torch.Size([1, 3, 640, 640])\n",
      "haze info: 611 619 14 10\n",
      "block info: 640 640\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 640, 640])\n",
      "trans_map shape: torch.Size([1, 3, 640, 640])\n",
      "\n",
      "Image: cityscape\n",
      "haze shape: torch.Size([1, 3, 608, 416])\n",
      "haze info: 600 400 4 8\n",
      "block info: 608 416\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 608, 416])\n",
      "trans_map shape: torch.Size([1, 3, 608, 416])\n",
      "\n",
      "Image: cliff\n",
      "haze shape: torch.Size([1, 3, 384, 576])\n",
      "haze info: 384 576 0 0\n",
      "block info: 384 576\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 384, 576])\n",
      "trans_map shape: torch.Size([1, 3, 384, 576])\n",
      "\n",
      "Image: forest\n",
      "haze shape: torch.Size([1, 3, 768, 1024])\n",
      "haze info: 768 1024 0 0\n",
      "block info: 768 1024\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 768, 1024])\n",
      "trans_map shape: torch.Size([1, 3, 768, 1024])\n",
      "\n",
      "Image: highquality13\n",
      "haze shape: torch.Size([1, 3, 576, 768])\n",
      "haze info: 576 768 0 0\n",
      "block info: 576 768\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 576, 768])\n",
      "trans_map shape: torch.Size([1, 3, 576, 768])\n",
      "\n",
      "Image: img33\n",
      "haze shape: torch.Size([1, 3, 768, 576])\n",
      "haze info: 768 576 0 0\n",
      "block info: 768 576\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "\n",
      "Image: img54\n",
      "haze shape: torch.Size([1, 3, 768, 1184])\n",
      "haze info: 763 1180 2 2\n",
      "block info: 768 1184\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 768, 1184])\n",
      "trans_map shape: torch.Size([1, 3, 768, 1184])\n",
      "\n",
      "Image: img69\n",
      "haze shape: torch.Size([1, 3, 1344, 2016])\n",
      "haze info: 1328 1999 8 8\n",
      "block info: 1344 2016\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 1344, 2016])\n",
      "trans_map shape: torch.Size([1, 3, 1344, 2016])\n",
      "\n",
      "Image: landscape\n",
      "haze shape: torch.Size([1, 3, 544, 608])\n",
      "haze info: 525 600 9 4\n",
      "block info: 544 608\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 544, 608])\n",
      "trans_map shape: torch.Size([1, 3, 544, 608])\n",
      "\n",
      "Image: lviv\n",
      "haze shape: torch.Size([1, 3, 1056, 1440])\n",
      "haze info: 1044 1428 6 6\n",
      "block info: 1056 1440\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 1056, 1440])\n",
      "trans_map shape: torch.Size([1, 3, 1056, 1440])\n",
      "\n",
      "Image: manhattan1\n",
      "haze shape: torch.Size([1, 3, 768, 576])\n",
      "haze info: 768 576 0 0\n",
      "block info: 768 576\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "\n",
      "Image: manhattan2\n",
      "haze shape: torch.Size([1, 3, 768, 1024])\n",
      "haze info: 768 1024 0 0\n",
      "block info: 768 1024\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 768, 1024])\n",
      "trans_map shape: torch.Size([1, 3, 768, 1024])\n",
      "\n",
      "Image: redbrickshouse\n",
      "haze shape: torch.Size([1, 3, 480, 448])\n",
      "haze info: 450 441 15 3\n",
      "block info: 480 448\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 480, 448])\n",
      "trans_map shape: torch.Size([1, 3, 480, 448])\n",
      "\n",
      "Image: road\n",
      "haze shape: torch.Size([1, 3, 416, 608])\n",
      "haze info: 400 600 8 4\n",
      "block info: 416 608\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 416, 608])\n",
      "trans_map shape: torch.Size([1, 3, 416, 608])\n",
      "\n",
      "Image: swans\n",
      "haze shape: torch.Size([1, 3, 416, 640])\n",
      "haze info: 416 624 0 8\n",
      "block info: 416 640\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 416, 640])\n",
      "trans_map shape: torch.Size([1, 3, 416, 640])\n",
      "\n",
      "Image: yosemite1\n",
      "haze shape: torch.Size([1, 3, 768, 576])\n",
      "haze info: 768 576 0 0\n",
      "block info: 768 576\n",
      "\n",
      "In Model Test: trans_map shape: torch.Size([1, 1, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "img_dir= join('./result0728_exp3_real_haze','full_pass1x1')\n",
    "block_num= opt.blocks\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "        for i, test_batched in enumerate(test_loader,0):\n",
    "            \n",
    "            print(\"Image:\",img_list[i])\n",
    "            # Prepare sample and target\n",
    "            # haze: (b,c,h,w) with (r,g,b) order\n",
    "            haze = test_batched['haze'][0]\n",
    "            h_origin,w_origin,h_pad,w_pad = test_batched['info']\n",
    "            h_origin,w_origin,h_pad,w_pad = h_origin.item(),w_origin.item(),h_pad.item(),w_pad.item()\n",
    "            h_block, w_block = test_batched['block_info']\n",
    "            h_block, w_block = h_block.item(), w_block.item()\n",
    "            print(\"haze shape:\", haze.shape)\n",
    "            print(\"haze info:\",h_origin,w_origin,h_pad,w_pad)\n",
    "            print(\"block info:\",h_block, w_block)\n",
    "            print()\n",
    "            \n",
    "            total_block_num,c,h,w = haze.shape\n",
    "            \n",
    "            trans_map, atmos_light,nonhaze_rec, dehaze = model(haze.to(device))\n",
    "            print(\"trans_map shape:\", trans_map.shape)\n",
    "                \n",
    "            trans_map_np = np.zeros(shape=(h*block_num,w*block_num,3))\n",
    "            atmos_map_np = np.zeros(shape=(h*block_num,w*block_num,3))\n",
    "            nonhaze_rec_np = np.zeros(shape=(h*block_num,w*block_num,3))\n",
    "            dehaze_np = np.zeros(shape=(h*block_num,w*block_num,3))\n",
    "            start_index=0\n",
    "            \n",
    "            for h_idx in range(block_num):\n",
    "                for w_idx in range(block_num):\n",
    "                    block = trans_map[start_index].cpu().numpy()\n",
    "                    block = np.transpose(block, (1,2,0))\n",
    "                    trans_map_np[h_idx*h_block:(h_idx+1)*h_block,w_idx*w_block:(w_idx+1)*w_block,:] = block\n",
    "                    \n",
    "                    block = atmos_light[start_index].cpu().numpy()\n",
    "                    block = np.transpose(block, (1,2,0))\n",
    "                    atmos_map_np[h_idx*h_block:(h_idx+1)*h_block,w_idx*w_block:(w_idx+1)*w_block,:] = block\n",
    "                    \n",
    "                    block = nonhaze_rec[start_index].cpu().numpy()\n",
    "                    block = np.transpose(block, (1,2,0))\n",
    "                    nonhaze_rec_np[h_idx*h_block:(h_idx+1)*h_block,w_idx*w_block:(w_idx+1)*w_block,:] = block\n",
    "\n",
    "                    block = dehaze[start_index].cpu().numpy()\n",
    "                    block = np.transpose(block, (1,2,0))\n",
    "                    dehaze_np[h_idx*h_block:(h_idx+1)*h_block,w_idx*w_block:(w_idx+1)*w_block,:] = block\n",
    "                    \n",
    "                    start_index += 1\n",
    "\n",
    "    \n",
    "            trans_map_np = trans_map_np[h_pad:h_pad+h_origin,w_pad:w_pad+w_origin,:]\n",
    "            atmos_map_np = atmos_map_np[h_pad:h_pad+h_origin,w_pad:w_pad+w_origin,:]\n",
    "            nonhaze_rec_np = nonhaze_rec_np[h_pad:h_pad+h_origin,w_pad:w_pad+w_origin,:]\n",
    "            dehaze_np = dehaze_np[h_pad:h_pad+h_origin,w_pad:w_pad+w_origin,:]\n",
    "            \n",
    "            trans_map_np = (255*trans_map_np).astype(np.uint8)[:,:,::-1]\n",
    "            atmos_map_np = (255*atmos_map_np).astype(np.uint8)[:,:,::-1]\n",
    "            nonhaze_rec_np = (255*nonhaze_rec_np).astype(np.uint8)[:,:,::-1]\n",
    "            dehaze_np = (255*dehaze_np).astype(np.uint8)[:,:,::-1]\n",
    "            \n",
    "#             print(\"trans dtype:\", trans_map_np.dtype)\n",
    "#             print(\"atmos dtype:\", atmos_map_np.dtype)\n",
    "#             print(\"nonohaze rec dtype:\", nonhaze_rec_np.dtype)\n",
    "#             print(\"dehaze dtype:\", dehaze_np.dtype)\n",
    "            \n",
    "#             cv2.imshow('Trans', trans_map_np)\n",
    "#             cv2.imshow('Atmos', atmos_map_np)\n",
    "#             cv2.imshow('Nonhaze', nonhaze_rec_np)\n",
    "#             cv2.imshow('Dehaze', dehaze_np)\n",
    "#             cv2.waitKey(0)\n",
    "#             cv2.destroyAllWindows()\n",
    "            \n",
    "            atmos_intensity_np = cv2.cvtColor(atmos_map_np, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "            # because opencv is BGR order, we need to change RGB to BGR\n",
    "            cv2.imwrite(join(img_dir,'dehaze',img_list[i]+'_dehaze.png'),dehaze_np)\n",
    "            cv2.imwrite(join(img_dir,'nonhaze_rec',img_list[i]+'_nonhaze_rec.png'),nonhaze_rec_np)\n",
    "            cv2.imwrite(join(img_dir,'trans',img_list[i]+'_trans.png'),trans_map_np)\n",
    "            cv2.imwrite(join(img_dir,'atmos',img_list[i]+'_atmos.png'),atmos_map_np)\n",
    "            cv2.imwrite(join(img_dir,'atmos_intensity',img_list[i] +'_atmos_intensity.png'),atmos_intensity_np)\n",
    "            print()\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blur atmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final atmos blending function version\n",
    "def blending_row(left,right, percent=0.5, max_value=1.0):\n",
    "    h,w,c = left.shape\n",
    "    width = int(w*percent)\n",
    "    width_max = (w-width)//2\n",
    "    width_min = w - width - width_max\n",
    "#     print(\"h,w,c:\", h,w,c)\n",
    "#     print(\"width:\", width)\n",
    "    left_mask = np.concatenate((np.linspace(max_value, max_value, num=width_max),np.linspace(max_value, 0., num=width)))\n",
    "    left_mask = np.concatenate((left_mask,np.linspace(0.0,0.0,num=width_min))).reshape((1,-1))\n",
    "    left_mask = np.repeat(left_mask, repeats=h, axis=0).reshape((h,-1,1))\n",
    "    right_mask = np.concatenate((np.linspace(0.0,0.0,num=width_min),np.linspace(0., max_value, num=width)))\n",
    "    right_mask = np.concatenate((right_mask,np.linspace(max_value, max_value, num=width_max))).reshape((1,-1))\n",
    "    right_mask = np.repeat(right_mask, repeats=h, axis=0).reshape((h,-1,1))\n",
    "#     print(left_mask.shape)\n",
    "#     print(right_mask.shape)\n",
    "    \n",
    "#     left_mask = cv2.GaussianBlur(left_mask,(51,51),25).reshape(left_mask.shape)\n",
    "#     right_mask = cv2.GaussianBlur(right_mask,(51,51),25).reshape(right_mask.shape)\n",
    "\n",
    "    new_left = left_mask*left\n",
    "    new_right = right_mask*right\n",
    "    result = new_left + new_right\n",
    "#     cv2.imshow('new left block', np.hstack((left, new_left)))\n",
    "#     cv2.imshow('new right block', np.hstack((right, new_right)))\n",
    "#     cv2.imshow('new result', result )\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    if np.max(result) > 1.0:\n",
    "        print(\"exceed boundary\")\n",
    "        result = np.clip(result,0.0,1.0)\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final atmos blending function version\n",
    "def blending_col(up,down, percent=0.5, max_value=1.0):\n",
    "    h,w,c = up.shape\n",
    "    height = int(h*percent)\n",
    "    height_max= (h-height)//2\n",
    "    height_min = h - height - height_max\n",
    "#     print(\"h,w,c:\", h,w,c)\n",
    "#     print(\"width:\", width)\n",
    "    up_mask = np.concatenate((np.linspace(max_value, max_value, num=height_max),np.linspace(max_value, 0., num=height)))\n",
    "    up_mask = np.concatenate((up_mask,np.linspace(0.0,0.0,num=height_min))).reshape((-1,1))\n",
    "    up_mask = np.repeat(up_mask, repeats=w, axis=1).reshape((-1,w,1))\n",
    "    down_mask = np.concatenate((np.linspace(0.0,0.0,num=height_min),np.linspace(0., max_value, num=height)))\n",
    "    down_mask = np.concatenate((down_mask,np.linspace(max_value, max_value, num=height_max))).reshape((-1,1))\n",
    "    down_mask = np.repeat(down_mask, repeats=w, axis=1).reshape((-1,w,1))\n",
    "    \n",
    "        \n",
    "#     up_mask = cv2.GaussianBlur(up_mask,(51,51),25).reshape(up_mask.shape)\n",
    "#     down_mask = cv2.GaussianBlur(down_mask,(51,51),25).reshape(down_mask.shape)\n",
    "    \n",
    "#     print(up_mask.shape)\n",
    "#     print(down_mask.shape)\n",
    "    new_up = up_mask*up\n",
    "    new_down = down_mask*down\n",
    "    result = new_up + new_down\n",
    "    \n",
    "#     cv2.imshow('new up block', np.hstack((up, new_up)))\n",
    "#     cv2.imshow('new down block', np.hstack((down, new_down)))\n",
    "#     cv2.imshow('new result', result )\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "    if np.max(result) > 1.0:\n",
    "        print(\"exceed boundary\")\n",
    "        result = np.clip(result,0.0,1.0)\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['./real_haze/haze/aerial_input.bmp', './real_haze/haze/castle_input.png', './real_haze/haze/cityscape_input.png', './real_haze/haze/cliff_input.jpg', './real_haze/haze/forest_input.png', './real_haze/haze/highquality13.png', './real_haze/haze/img33.png', './real_haze/haze/img54.jpg', './real_haze/haze/img69.jpg', './real_haze/haze/landscape_input.jpg', './real_haze/haze/lviv_input.png', './real_haze/haze/manhattan1_input.jpg', './real_haze/haze/manhattan2_input.png', './real_haze/haze/redbrickshouse_input.bmp', './real_haze/haze/road_input.png', './real_haze/haze/swans_input.png', './real_haze/haze/yosemite1_input.png']\n"
     ]
    }
   ],
   "source": [
    "haze_dir = './real_haze/haze/'\n",
    "haze_name = []\n",
    "for root, dirs, files in os.walk(haze_dir):\n",
    "    for file in files:\n",
    "#         print(file)\n",
    "        haze_name.append(join(haze_dir,file))\n",
    "\n",
    "print(len(haze_name))\n",
    "print(haze_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['./result0728_exp3_real_haze/full_pass3x3/atmos/aerial_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/castle_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/cityscape_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/cliff_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/forest_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/highquality13_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/img33_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/img54_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/img69_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/landscape_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/lviv_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/manhattan1_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/manhattan2_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/redbrickshouse_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/road_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/swans_atmos.png', './result0728_exp3_real_haze/full_pass3x3/atmos/yosemite1_atmos.png']\n"
     ]
    }
   ],
   "source": [
    "atmos_dir = './result0728_exp3_real_haze/full_pass3x3/atmos/'\n",
    "atmos_name = []\n",
    "for root, dirs, files in os.walk(atmos_dir):\n",
    "    for file in files:\n",
    "#         print(file)\n",
    "        atmos_name.append(join(atmos_dir,file))\n",
    "\n",
    "print(len(atmos_name))\n",
    "print(atmos_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: (442, 622, 19, 25)\n",
      "(480, 672, 3)\n",
      "h_block, w_block: 160 224\n",
      "final atmos: (480, 672, 3)\n",
      "info: (611, 619, 30, 26)\n",
      "(672, 672, 3)\n",
      "h_block, w_block: 224 224\n",
      "final atmos: (672, 672, 3)\n",
      "info: (600, 400, 36, 40)\n",
      "(672, 480, 3)\n",
      "h_block, w_block: 224 160\n",
      "final atmos: (672, 480, 3)\n",
      "info: (384, 576, 0, 0)\n",
      "(384, 576, 3)\n",
      "h_block, w_block: 128 192\n",
      "final atmos: (384, 576, 3)\n",
      "info: (768, 1024, 0, 16)\n",
      "(768, 1056, 3)\n",
      "h_block, w_block: 256 352\n",
      "final atmos: (768, 1056, 3)\n",
      "info: (576, 768, 0, 0)\n",
      "(576, 768, 3)\n",
      "h_block, w_block: 192 256\n",
      "final atmos: (576, 768, 3)\n",
      "info: (768, 576, 0, 0)\n",
      "(768, 576, 3)\n",
      "h_block, w_block: 256 192\n",
      "final atmos: (768, 576, 3)\n",
      "info: (763, 1180, 2, 34)\n",
      "(768, 1248, 3)\n",
      "h_block, w_block: 256 416\n",
      "final atmos: (768, 1248, 3)\n",
      "info: (1328, 1999, 8, 8)\n",
      "(1344, 2016, 3)\n",
      "h_block, w_block: 448 672\n",
      "final atmos: (1344, 2016, 3)\n",
      "info: (525, 600, 25, 36)\n",
      "(576, 672, 3)\n",
      "h_block, w_block: 192 224\n",
      "final atmos: (576, 672, 3)\n",
      "info: (1044, 1428, 6, 6)\n",
      "(1056, 1440, 3)\n",
      "h_block, w_block: 352 480\n",
      "final atmos: (1056, 1440, 3)\n",
      "info: (768, 576, 0, 0)\n",
      "(768, 576, 3)\n",
      "h_block, w_block: 256 192\n",
      "final atmos: (768, 576, 3)\n",
      "info: (768, 1024, 0, 16)\n",
      "(768, 1056, 3)\n",
      "h_block, w_block: 256 352\n",
      "final atmos: (768, 1056, 3)\n",
      "info: (450, 441, 15, 19)\n",
      "(480, 480, 3)\n",
      "h_block, w_block: 160 160\n",
      "final atmos: (480, 480, 3)\n",
      "info: (400, 600, 40, 36)\n",
      "(480, 672, 3)\n",
      "h_block, w_block: 160 224\n",
      "final atmos: (480, 672, 3)\n",
      "info: (416, 624, 32, 24)\n",
      "(480, 672, 3)\n",
      "h_block, w_block: 160 224\n",
      "final atmos: (480, 672, 3)\n",
      "info: (768, 576, 0, 0)\n",
      "(768, 576, 3)\n",
      "h_block, w_block: 256 192\n",
      "final atmos: (768, 576, 3)\n"
     ]
    }
   ],
   "source": [
    "# original method\n",
    "dst_dir = './result0728_exp3_real_haze/full_pass3x3/'\n",
    "# img_ratio=160 # 5x5 blocks\n",
    "img_ratio = 32*3 # 3x3 blocks\n",
    "# num_block=5\n",
    "num_block=3 # 3x3 blocks\n",
    "for k in range(len(haze_name)):\n",
    "    haze = cv2.imread(haze_name[k])\n",
    "#     print(haze.shape)\n",
    "    h,w,c = haze.shape\n",
    "    h_pad1, h_pad2 = 0,0\n",
    "    w_pad1, w_pad2= 0,0\n",
    "    if h%img_ratio != 0:\n",
    "        pad= img_ratio *(h//img_ratio+1)-h\n",
    "        h_pad1, h_pad2 = pad//2, (pad-pad//2)\n",
    "    if w%img_ratio != 0:\n",
    "        pad= img_ratio*(w//img_ratio+1)-w\n",
    "        w_pad1, w_pad2 = pad//2, (pad-pad//2)\n",
    "    info = (h,w,h_pad1,w_pad1)\n",
    "    print(\"info:\", info)\n",
    "    # (b,g,r) order with value [0,1]\n",
    "    atmos = cv2.imread(atmos_name[k])\n",
    "#     atmos = cv2.GaussianBlur(atmos,(51,51),25).reshape(atmos.shape)\n",
    "    atmos = atmos/255.\n",
    "    atmos_pad = np.pad(atmos, ((h_pad1, h_pad2),(w_pad1, w_pad2),(0,0)), mode='symmetric')\n",
    "    print(atmos_pad.shape)\n",
    "    # h_pad,w_pad,c_pad = atmos_pad.shape\n",
    "    h_block, w_block = atmos_pad.shape[0]//num_block, atmos_pad.shape[1]//num_block\n",
    "    print(\"h_block, w_block:\", h_block, w_block)\n",
    "    \n",
    "    \n",
    "    new_atmos = atmos_pad.copy()\n",
    "#     print(\"new atmos:\", new_atmos.shape)\n",
    "#     cv2.imshow('new_atmos', new_atmos)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()   \n",
    "    \n",
    "    for i in range(num_block):\n",
    "        for j in range(num_block-1):\n",
    "            left = atmos_pad[i*h_block:(i+1)*h_block, j*w_block:(j+1)*w_block,:]\n",
    "            right = atmos_pad[i*h_block:(i+1)*h_block, (j+1)*w_block:(j+2)*w_block,:]\n",
    "            new_atmos[i*h_block:(i+1)*h_block,w_block//2+j*w_block:w_block//2+(j+1)*w_block,:] = blending_row(left,right,percent=0.5, max_value=1.0)\n",
    "            \n",
    "    \n",
    "    \n",
    "    final_atmos = new_atmos.copy()\n",
    "    print(\"final atmos:\", final_atmos.shape)\n",
    "    for i in range(num_block-1):\n",
    "        up = new_atmos[i*h_block:(i+1)*h_block,:,:]\n",
    "        down = new_atmos[(i+1)*h_block:(i+2)*h_block,:,:]\n",
    "        final_atmos[h_block//2+i*h_block:h_block//2+(i+1)*h_block,:,:] = blending_col(up,down,percent=0.5,max_value=1.0)\n",
    "\n",
    "    final_atmos_blur = cv2.GaussianBlur(final_atmos,(51,51),25).reshape(final_atmos.shape)\n",
    "#     final_atmos_blur = cv2.GaussianBlur(final_atmos_blur,(51,51),25).reshape(final_atmos.shape)\n",
    "            \n",
    "#     cv2.imshow('original', atmos_pad)\n",
    "#     cv2.imshow('new', new_atmos)\n",
    "#     cv2.imshow('zero', zeros)\n",
    "#     cv2.imshow('atmos', atmos_pad)\n",
    "#     cv2.imshow('blending row', new_atmos)\n",
    "#     cv2.imshow('final', final_atmos)\n",
    "#     cv2.imshow('final blur', final_atmos_blur)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    final_atmos = (255*final_atmos[h_pad1:h_pad1+h,w_pad1:w_pad1+w,:]).astype(np.uint8)\n",
    "    final_atmos_blur = (255*final_atmos_blur[h_pad1:h_pad1+h,w_pad1:w_pad1+w,:]).astype(np.uint8)\n",
    "         \n",
    "    cv2.imwrite(join(dst_dir,'atmos_linear_blur',img_list[k]+'_atmos_blur.png'),final_atmos)\n",
    "    cv2.imwrite(join(dst_dir,'atmos_linear_blur_gaussian',img_list[k]+'_atmos_blur.png'),final_atmos_blur)\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atmos_dir = './result0728_exp3_real_haze/full_pass3x3/atmos_linear_blur/'\n",
    "# atmos_name = []\n",
    "# for root, dirs, files in os.walk(atmos_dir):\n",
    "#     for file in files:\n",
    "# #         print(file)\n",
    "#         atmos_name.append(join(atmos_dir,file))\n",
    "\n",
    "# print(len(atmos_name))\n",
    "# print(atmos_name[:3])\n",
    "\n",
    "\n",
    "# atmos_origin_dir = './result0728_exp3_real_haze/full_pass3x3/atmos/'\n",
    "# atmos_origin_name = []\n",
    "# for root, dirs, files in os.walk(atmos_origin_dir):\n",
    "#     for file in files:\n",
    "# #         print(file)\n",
    "#         atmos_origin_name.append(join(atmos_origin_dir,file))\n",
    "\n",
    "# print(len(atmos_origin_name))\n",
    "# print(atmos_origin_name[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_ratio = 32*3 # 3x3 blocks\n",
    "# block_num=3 # 3x3 blocks\n",
    "# dst_dir = './result0728_exp3_real_haze/full_pass3x3/'\n",
    "\n",
    "# for k in range(len(atmos_name)):\n",
    "#     # (b,g,r) order with value [0,1]\n",
    "#     atmos = cv2.imread(atmos_name[k])/255.\n",
    "#     atmos_origin = cv2.imread(atmos_origin_name[k])/255.\n",
    "#     h,w,c = atmos.shape\n",
    "#     new_h, new_w = img_ratio*math.ceil(h/img_ratio), img_ratio*math.ceil(w/img_ratio)\n",
    "#     h_block, w_block = (new_h//block_num), (new_w//block_num)\n",
    "    \n",
    "#     h_pad1, h_pad2 = 0,0\n",
    "#     w_pad1, w_pad2= 0,0\n",
    "#     if new_h != h:\n",
    "#         pad= new_h-h\n",
    "#         h_pad1, h_pad2 = pad//2, (pad-pad//2)\n",
    "#     if new_w != w:\n",
    "#         pad= new_w-w\n",
    "#         w_pad1, w_pad2 = pad//2, (pad-pad//2)\n",
    "    \n",
    "#     info = (h,w,h_pad1,w_pad1)\n",
    "#     block_info = (h_block, w_block)\n",
    "#     print(\"info:\", info)\n",
    "#     print(\"block info:\", block_info)\n",
    "#     atmos_pad = np.pad(atmos, ((h_pad1, h_pad2),(w_pad1, w_pad2),(0,0)), mode='symmetric')\n",
    "#     atmos_origin_pad = np.pad(atmos_origin, ((h_pad1, h_pad2),(w_pad1, w_pad2),(0,0)), mode='symmetric')\n",
    "#     print(\"amos_pad:\", atmos_pad.shape)\n",
    "    \n",
    "    \n",
    "#     new_atmos = atmos_origin_pad.copy()\n",
    "#     width = 50\n",
    "#     start_idx = 5//2\n",
    "#     # for each row\n",
    "#     for i in range(1,num_block):\n",
    "#             new_atmos[i*h_block-start_idx:i*h_block-start_idx+width,:,:] = atmos_pad[i*h_block-start_idx:i*h_block-start_idx+width,:,:]\n",
    "#     # for each col\n",
    "#     for i in range(1,num_block):\n",
    "#             new_atmos[:,i*w_block-start_idx:i*w_block-start_idx+width,:] = atmos_pad[:,i*w_block-start_idx:i*w_block-start_idx+width,:]\n",
    "\n",
    "    \n",
    "#     cv2.imshow('original atmos', atmos_origin_pad)\n",
    "#     cv2.imshow('new atmos', new_atmos)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "#     new_atmos = (255*new_atmos[h_pad1:h_pad1+h,w_pad1:w_pad1+w,:]).astype(np.uint8)\n",
    "#     cv2.imwrite(join(dst_dir,'atmos_final',img_list[k]+'_atmos_blur.png'),new_atmos)\n",
    "    \n",
    "# #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3x3 + blending atmos (failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def CreateMask_atmos(h_block,w_block,num_block=3, max_value=0.9, overlap=False):\n",
    "\n",
    "#     new_h, new_w = h_block*num_block, w_block*num_block\n",
    "#     repeats = num_block-1\n",
    "#     height, width = h_block//2, w_block//2\n",
    "#     if overlap:\n",
    "#         new_h, new_w = new_h+h_block, new_w + w_block\n",
    "#         repeats = num_block\n",
    "        \n",
    "#     trans_mask = np.zeros((new_h,new_w,1))\n",
    "    \n",
    "# #     max_mask = np.linspace(max_value,max_value, num=16)\n",
    "#     min_mask = np.linspace(0.0,0.0, num=16)\n",
    "    \n",
    "#    # 3x3 mask\n",
    "#     col_mask = np.concatenate((np.linspace(max_value, 0.,num=w_block//2),np.linspace(0., max_value,num=w_block//2)))\n",
    "#     col_mask = np.tile(col_mask,reps=repeats)\n",
    "#     col_mask = np.concatenate((np.linspace(max_value,max_value, num=width),col_mask))\n",
    "#     col_mask = np.concatenate((col_mask,np.linspace(max_value,max_value, num=width)))\n",
    "#     print(\"col_mask:\", col_mask.shape)\n",
    "    \n",
    "#     row_mask = np.concatenate((np.linspace(max_value, 0.,num=h_block//2),np.linspace(0., max_value,num=h_block//2)))\n",
    "#     row_mask = np.tile(row_mask,reps=repeats)\n",
    "#     row_mask = np.concatenate((np.linspace(max_value,max_value, num=height),row_mask))\n",
    "#     row_mask = np.concatenate((row_mask,np.linspace(max_value,max_value, num=height)))\n",
    "#     print(\"row mask:\", row_mask.shape)\n",
    "                              \n",
    "#     for i in range(trans_mask.shape[0]):\n",
    "#         for j in range(trans_mask.shape[1]):\n",
    "#             trans_mask[i,j] = min(row_mask[i], col_mask[j])\n",
    "    \n",
    "#     col_mask = np.repeat(col_mask.reshape((-1,new_w,1)), repeats=new_h, axis=0)\n",
    "#     row_mask = np.repeat(row_mask.reshape((new_h,-1,1)), repeats=new_w, axis=1)\n",
    "#     trans_mask_blur = cv2.GaussianBlur(trans_mask,(51,51),25).reshape((new_h,new_w,1))\n",
    "    \n",
    "# #     cv2.imshow('col mask', col_mask)\n",
    "# #     cv2.imshow('row mask', row_mask)\n",
    "# #     cv2.imshow('mask', trans_mask)\n",
    "# #     cv2.imshow('mask blur', trans_mask_blur)\n",
    "# #     cv2.waitKey(0)\n",
    "# #     cv2.destroyAllWindows()\n",
    "#     return trans_mask_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trans_dir = './result0728_exp3_real_haze/full_pass3x3/trans/'\n",
    "# trans_dir = './result0728_exp3_real_haze/full_pass3x3/atmos/'\n",
    "# trans_name = []\n",
    "# for root, dirs, files in os.walk(trans_dir):\n",
    "#     for file in files:\n",
    "# #         print(file)\n",
    "#         trans_name.append(join(trans_dir,file))\n",
    "\n",
    "# print(len(trans_name))\n",
    "# print(trans_name[:3])\n",
    "\n",
    "# # self.data_files[key].sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "# # trans_name.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "# # print(trans_name[:3])\n",
    "\n",
    "# # trans_origin_dir = './result0728_exp3_real_haze/full_pass1x1/trans/'\n",
    "# trans_origin_dir = './result0728_exp3_real_haze/full_pass3x3/atmos_linear_blur/'\n",
    "# # trans_origin_dir = './result0728_exp3_real_haze/full_pass3x3/atmos_linear_blur/'\n",
    "# trans_origin_name = []\n",
    "# for root, dirs, files in os.walk(trans_origin_dir):\n",
    "#     for file in files:\n",
    "# #         print(file)\n",
    "#         trans_origin_name.append(join(trans_origin_dir,file))\n",
    "\n",
    "# print(len(trans_origin_name))\n",
    "# print(trans_origin_name[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # img_ratio=160 # 5x5 blocks\n",
    "# img_ratio = 32*3 # 3x3 blocks\n",
    "# # num_block=5\n",
    "# block_num=3 # 3x3 blocks\n",
    "# dst_dir = './result0728_exp3_real_haze/full_pass3x3/'\n",
    "\n",
    "# for k in range(len(trans_name)):\n",
    "#     # (b,g,r) order with value [0,1]\n",
    "#     trans = cv2.imread(trans_name[k])/255.\n",
    "#     trans_origin = cv2.imread(trans_origin_name[k])/255.\n",
    "#     h,w,c = trans.shape\n",
    "#     new_h, new_w = img_ratio*math.ceil(h/img_ratio), img_ratio*math.ceil(w/img_ratio)\n",
    "#     h_block, w_block = (new_h//block_num), (new_w//block_num)\n",
    "    \n",
    "#     h_pad1, h_pad2 = 0,0\n",
    "#     w_pad1, w_pad2= 0,0\n",
    "#     if new_h != h:\n",
    "#         pad= new_h-h\n",
    "#         h_pad1, h_pad2 = pad//2, (pad-pad//2)\n",
    "#     if new_w != w:\n",
    "#         pad= new_w-w\n",
    "#         w_pad1, w_pad2 = pad//2, (pad-pad//2)\n",
    "    \n",
    "#     info = (h,w,h_pad1,w_pad1)\n",
    "#     block_info = (h_block, w_block)\n",
    "#     print(\"info:\", info)\n",
    "#     print(\"block info:\", block_info)\n",
    "#     trans_pad = np.pad(trans, ((h_pad1, h_pad2),(w_pad1, w_pad2),(0,0)), mode='symmetric')\n",
    "#     trans_origin_pad = np.pad(trans_origin, ((h_pad1, h_pad2),(w_pad1, w_pad2),(0,0)), mode='symmetric')\n",
    "#     print(\"trans_pad:\", trans_pad.shape)\n",
    "    \n",
    "    \n",
    "#     new_trans = trans_pad.copy()\n",
    "#     trans_mask = np.zeros((new_h,new_w,1))\n",
    "#     print(\"trans mask:\", trans_mask.shape)\n",
    "            \n",
    "\n",
    "#     trans_mask_blur = CreateMask_atmos(h_block, w_block, num_block=3, max_value=1.0)\n",
    "#     new_result = trans_mask_blur*trans_pad + (1-trans_mask_blur)*trans_origin_pad\n",
    "    \n",
    "# #     cv2.imshow('mask blur', trans_mask_blur)\n",
    "# #     cv2.imshow('new_result', new_result)\n",
    "# #     cv2.waitKey(0)\n",
    "# #     cv2.destroyAllWindows()\n",
    "#     new_result = (255*new_result[h_pad1:h_pad1+h,w_pad1:w_pad1+w,:]).astype(np.uint8)\n",
    "#     mask = (255*trans_mask_blur[h_pad1:h_pad1+h,w_pad1:w_pad1+w,:]).astype(np.uint8)\n",
    "#     reverse_mask = 255-mask\n",
    "#     cv2.imwrite(join(dst_dir,'atmos_blur_final',img_list[k]+'_atmos_blur.png'),new_result)\n",
    "#     cv2.imwrite(join(dst_dir,'atmos_blur_final_mask',img_list[k]+'_mask.png'),mask)\n",
    "#     cv2.imwrite(join(dst_dir,'atmos_blur_final_reverse_mask',img_list[k]+'_reverse_mask.png'),reverse_mask)\n",
    "    \n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3x3 + 1x1 trans blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateMask(h_block,w_block,num_block=3, max_value=0.9, overlap=False):\n",
    "\n",
    "    new_h, new_w = h_block*num_block, w_block*num_block\n",
    "    repeats = num_block-1\n",
    "    height, width = h_block//2, w_block//2\n",
    "    if overlap:\n",
    "        new_h, new_w = new_h+h_block, new_w + w_block\n",
    "        repeats = num_block\n",
    "        \n",
    "    trans_mask = np.zeros((new_h,new_w,1))\n",
    "    \n",
    "#     max_mask = np.linspace(max_value,max_value, num=16)\n",
    "    min_mask = np.linspace(0.0,0.0, num=16)\n",
    "    \n",
    "   # 3x3 mask\n",
    "    col_mask = np.concatenate((np.linspace(max_value, 0.,num=w_block//2),np.linspace(0., max_value,num=w_block//2)))\n",
    "    col_mask = np.tile(col_mask,reps=repeats)\n",
    "    col_mask = np.concatenate((np.linspace(max_value,max_value, num=width),col_mask))\n",
    "    col_mask = np.concatenate((col_mask,np.linspace(max_value,max_value, num=width)))\n",
    "    print(\"col_mask:\", col_mask.shape)\n",
    "    \n",
    "    row_mask = np.concatenate((np.linspace(max_value, 0.,num=h_block//2),np.linspace(0., max_value,num=h_block//2)))\n",
    "    row_mask = np.tile(row_mask,reps=repeats)\n",
    "    row_mask = np.concatenate((np.linspace(max_value,max_value, num=height),row_mask))\n",
    "    row_mask = np.concatenate((row_mask,np.linspace(max_value,max_value, num=height)))\n",
    "    print(\"row mask:\", row_mask.shape)\n",
    "                              \n",
    "    for i in range(trans_mask.shape[0]):\n",
    "        for j in range(trans_mask.shape[1]):\n",
    "            trans_mask[i,j] = min(row_mask[i], col_mask[j])\n",
    "    \n",
    "    col_mask = np.repeat(col_mask.reshape((-1,new_w,1)), repeats=new_h, axis=0)\n",
    "    row_mask = np.repeat(row_mask.reshape((new_h,-1,1)), repeats=new_w, axis=1)\n",
    "    trans_mask_blur = cv2.GaussianBlur(trans_mask,(51,51),25).reshape((new_h,new_w,1))\n",
    "    \n",
    "#     cv2.imshow('col mask', col_mask)\n",
    "#     cv2.imshow('row mask', row_mask)\n",
    "#     cv2.imshow('mask', trans_mask)\n",
    "#     cv2.imshow('mask blur', trans_mask_blur)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    return trans_mask_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['./result0728_exp3_real_haze/full_pass3x3/trans/aerial_trans.png', './result0728_exp3_real_haze/full_pass3x3/trans/castle_trans.png', './result0728_exp3_real_haze/full_pass3x3/trans/cityscape_trans.png']\n",
      "17\n",
      "['./result0728_exp3_real_haze/full_pass1x1/trans/aerial_trans.png', './result0728_exp3_real_haze/full_pass1x1/trans/castle_trans.png', './result0728_exp3_real_haze/full_pass1x1/trans/cityscape_trans.png']\n"
     ]
    }
   ],
   "source": [
    "trans_dir = './result0728_exp3_real_haze/full_pass3x3/trans/'\n",
    "trans_name = []\n",
    "for root, dirs, files in os.walk(trans_dir):\n",
    "    for file in files:\n",
    "#         print(file)\n",
    "        trans_name.append(join(trans_dir,file))\n",
    "\n",
    "print(len(trans_name))\n",
    "print(trans_name[:3])\n",
    "\n",
    "# self.data_files[key].sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "# trans_name.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "# print(trans_name[:3])\n",
    "\n",
    "trans_origin_dir = './result0728_exp3_real_haze/full_pass1x1/trans/'\n",
    "trans_origin_name = []\n",
    "for root, dirs, files in os.walk(trans_origin_dir):\n",
    "    for file in files:\n",
    "#         print(file)\n",
    "        trans_origin_name.append(join(trans_origin_dir,file))\n",
    "\n",
    "print(len(trans_origin_name))\n",
    "print(trans_origin_name[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: (442, 622, 19, 25)\n",
      "block info: (160, 224)\n",
      "trans_pad: (480, 672, 3)\n",
      "trans mask: (480, 672, 1)\n",
      "col_mask: (672,)\n",
      "row mask: (480,)\n",
      "info: (611, 619, 30, 26)\n",
      "block info: (224, 224)\n",
      "trans_pad: (672, 672, 3)\n",
      "trans mask: (672, 672, 1)\n",
      "col_mask: (672,)\n",
      "row mask: (672,)\n",
      "info: (600, 400, 36, 40)\n",
      "block info: (224, 160)\n",
      "trans_pad: (672, 480, 3)\n",
      "trans mask: (672, 480, 1)\n",
      "col_mask: (480,)\n",
      "row mask: (672,)\n",
      "info: (384, 576, 0, 0)\n",
      "block info: (128, 192)\n",
      "trans_pad: (384, 576, 3)\n",
      "trans mask: (384, 576, 1)\n",
      "col_mask: (576,)\n",
      "row mask: (384,)\n",
      "info: (768, 1024, 0, 16)\n",
      "block info: (256, 352)\n",
      "trans_pad: (768, 1056, 3)\n",
      "trans mask: (768, 1056, 1)\n",
      "col_mask: (1056,)\n",
      "row mask: (768,)\n",
      "info: (576, 768, 0, 0)\n",
      "block info: (192, 256)\n",
      "trans_pad: (576, 768, 3)\n",
      "trans mask: (576, 768, 1)\n",
      "col_mask: (768,)\n",
      "row mask: (576,)\n",
      "info: (768, 576, 0, 0)\n",
      "block info: (256, 192)\n",
      "trans_pad: (768, 576, 3)\n",
      "trans mask: (768, 576, 1)\n",
      "col_mask: (576,)\n",
      "row mask: (768,)\n",
      "info: (763, 1180, 2, 34)\n",
      "block info: (256, 416)\n",
      "trans_pad: (768, 1248, 3)\n",
      "trans mask: (768, 1248, 1)\n",
      "col_mask: (1248,)\n",
      "row mask: (768,)\n",
      "info: (1328, 1999, 8, 8)\n",
      "block info: (448, 672)\n",
      "trans_pad: (1344, 2016, 3)\n",
      "trans mask: (1344, 2016, 1)\n",
      "col_mask: (2016,)\n",
      "row mask: (1344,)\n",
      "info: (525, 600, 25, 36)\n",
      "block info: (192, 224)\n",
      "trans_pad: (576, 672, 3)\n",
      "trans mask: (576, 672, 1)\n",
      "col_mask: (672,)\n",
      "row mask: (576,)\n",
      "info: (1044, 1428, 6, 6)\n",
      "block info: (352, 480)\n",
      "trans_pad: (1056, 1440, 3)\n",
      "trans mask: (1056, 1440, 1)\n",
      "col_mask: (1440,)\n",
      "row mask: (1056,)\n",
      "info: (768, 576, 0, 0)\n",
      "block info: (256, 192)\n",
      "trans_pad: (768, 576, 3)\n",
      "trans mask: (768, 576, 1)\n",
      "col_mask: (576,)\n",
      "row mask: (768,)\n",
      "info: (768, 1024, 0, 16)\n",
      "block info: (256, 352)\n",
      "trans_pad: (768, 1056, 3)\n",
      "trans mask: (768, 1056, 1)\n",
      "col_mask: (1056,)\n",
      "row mask: (768,)\n",
      "info: (450, 441, 15, 19)\n",
      "block info: (160, 160)\n",
      "trans_pad: (480, 480, 3)\n",
      "trans mask: (480, 480, 1)\n",
      "col_mask: (480,)\n",
      "row mask: (480,)\n",
      "info: (400, 600, 40, 36)\n",
      "block info: (160, 224)\n",
      "trans_pad: (480, 672, 3)\n",
      "trans mask: (480, 672, 1)\n",
      "col_mask: (672,)\n",
      "row mask: (480,)\n",
      "info: (416, 624, 32, 24)\n",
      "block info: (160, 224)\n",
      "trans_pad: (480, 672, 3)\n",
      "trans mask: (480, 672, 1)\n",
      "col_mask: (672,)\n",
      "row mask: (480,)\n",
      "info: (768, 576, 0, 0)\n",
      "block info: (256, 192)\n",
      "trans_pad: (768, 576, 3)\n",
      "trans mask: (768, 576, 1)\n",
      "col_mask: (576,)\n",
      "row mask: (768,)\n"
     ]
    }
   ],
   "source": [
    "# img_ratio=160 # 5x5 blocks\n",
    "img_ratio = 32*3 # 3x3 blocks\n",
    "# num_block=5\n",
    "block_num=3 # 3x3 blocks\n",
    "dst_dir = './result0728_exp3_real_haze/full_pass3x3/'\n",
    "\n",
    "for k in range(len(trans_name)):\n",
    "    # (b,g,r) order with value [0,1]\n",
    "    trans = cv2.imread(trans_name[k])/255.\n",
    "    trans_origin = cv2.imread(trans_origin_name[k])/255.\n",
    "    h,w,c = trans.shape\n",
    "    new_h, new_w = img_ratio*math.ceil(h/img_ratio), img_ratio*math.ceil(w/img_ratio)\n",
    "    h_block, w_block = (new_h//block_num), (new_w//block_num)\n",
    "    \n",
    "    h_pad1, h_pad2 = 0,0\n",
    "    w_pad1, w_pad2= 0,0\n",
    "    if new_h != h:\n",
    "        pad= new_h-h\n",
    "        h_pad1, h_pad2 = pad//2, (pad-pad//2)\n",
    "    if new_w != w:\n",
    "        pad= new_w-w\n",
    "        w_pad1, w_pad2 = pad//2, (pad-pad//2)\n",
    "    \n",
    "    info = (h,w,h_pad1,w_pad1)\n",
    "    block_info = (h_block, w_block)\n",
    "    print(\"info:\", info)\n",
    "    print(\"block info:\", block_info)\n",
    "    trans_pad = np.pad(trans, ((h_pad1, h_pad2),(w_pad1, w_pad2),(0,0)), mode='symmetric')\n",
    "    trans_origin_pad = np.pad(trans_origin, ((h_pad1, h_pad2),(w_pad1, w_pad2),(0,0)), mode='symmetric')\n",
    "    print(\"trans_pad:\", trans_pad.shape)\n",
    "    \n",
    "    \n",
    "    new_trans = trans_pad.copy()\n",
    "    trans_mask = np.zeros((new_h,new_w,1))\n",
    "    print(\"trans mask:\", trans_mask.shape)\n",
    "            \n",
    "\n",
    "    trans_mask_blur = CreateMask(h_block, w_block, num_block=3, max_value=0.7)\n",
    "    new_result = trans_mask_blur*trans_pad + (1-trans_mask_blur)*trans_origin_pad\n",
    "    \n",
    "#     cv2.imshow('mask blur', trans_mask_blur)\n",
    "#     cv2.imshow('new_result', new_result)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    new_result = (255*new_result[h_pad1:h_pad1+h,w_pad1:w_pad1+w,:]).astype(np.uint8)\n",
    "    mask = (255*trans_mask_blur[h_pad1:h_pad1+h,w_pad1:w_pad1+w,:]).astype(np.uint8)\n",
    "    reverse_mask = 255-mask\n",
    "    cv2.imwrite(join(dst_dir,'trans_blur_final',img_list[k]+'_trans_blur.png'),new_result)\n",
    "    cv2.imwrite(join(dst_dir,'trans_blur_final_mask',img_list[k]+'_mask.png'),mask)\n",
    "    cv2.imwrite(join(dst_dir,'trans_blur_final_reverse_mask',img_list[k]+'_reverse_mask.png'),reverse_mask)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global trans + 3x3 local atmos blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original testset + overlap\n",
    "class CreateTestDataSet_Dehaze(Dataset):\n",
    "    def __init__(self, test_dir, block_num = 3,transform=None, overlap=False):\n",
    "        self.dir= test_dir\n",
    "        self.transform=transform\n",
    "        self.data_files = {'haze':[]}\n",
    "        self.img_ratio= lcm(32,block_num)\n",
    "        self.block_num = block_num\n",
    "        self.overlap=overlap\n",
    "#         self.data_files = {'haze':[],'GT':[]}\n",
    "\n",
    "        for key in self.data_files.keys():\n",
    "            subdir = join(self.dir, key)\n",
    "            self.data_files[key] += [join(subdir,x) for x in listdir(subdir) if is_image_file(x) ]\n",
    "            # self.data_files[key].sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        haze_name = self.data_files['haze'][index]\n",
    "        haze = Image.open(haze_name).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            # apply transform to each sample in data_files\n",
    "            haze = self.transform(haze)\n",
    "            \n",
    "        c,h,w = haze.shape\n",
    "        new_h, new_w = self.img_ratio*math.ceil(h/self.img_ratio), self.img_ratio*math.ceil(w/self.img_ratio)\n",
    "        h_block,w_block = (new_h//self.block_num), (new_w//self.block_num)\n",
    "        if self.overlap:\n",
    "            new_h, new_w = new_h+h_block, new_w+w_block\n",
    "        \n",
    "        h_pad1, h_pad2 = 0,0\n",
    "        w_pad1, w_pad2= 0,0\n",
    "        if new_h != h:\n",
    "            pad= new_h-h\n",
    "            h_pad1, h_pad2 = pad//2, (pad-pad//2)\n",
    "        if new_w != w:\n",
    "            pad= new_w-w\n",
    "            w_pad1, w_pad2 = pad//2, (pad-pad//2)\n",
    "            \n",
    "        haze_pad = F.pad(haze.unsqueeze(0), (w_pad1, w_pad2, h_pad1, h_pad2), mode='reflect').squeeze(0)\n",
    "#         all_blocks = view_as_blocks(haze_pad.numpy(), block_shape=(c,h_block,w_block)).reshape(-1,c,h_block,w_block)\n",
    "#         print(\"all blocks size\", all_blocks.shape)\n",
    "        # all_blocks: (total_block_num, channel, h, w)\n",
    "#         all_blocks = torch.from_numpy(all_blocks)\n",
    "        \n",
    "        info = (h,w,h_pad1,w_pad1)\n",
    "        block_info = (h_block, w_block)      \n",
    "        sample={'haze': haze_pad, 'info': info, 'block_info': block_info}       \n",
    "        return sample\n",
    "    def __len__(self):\n",
    "        return len(self.data_files['haze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestLoader_Dehaze(test_dir, blocks=3, overlap=False):\n",
    "    # val_test_transform = transforms.Resize((image_size,image_size))\n",
    "    test_transform = transforms.Compose([\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))\n",
    "                                             # Scale((image_size, image_size), use_trans_atmos=trans_atmos),\n",
    "#                                              ToTensor(use_trans_atmos = trans_atmos),\n",
    "#                                              Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],use_trans_atmos = trans_atmos)\n",
    "                                        ])\n",
    "    test_set = CreateTestDataSet_Dehaze(test_dir, blocks,test_transform, overlap)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "    return test_loader, test_set.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_dehaze(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_dehaze, self).__init__()\n",
    "        self.generate_dehaze = refinement_final()\n",
    "        self.unnormalize_fun = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "\n",
    "    def forward(self, x, trans_map, atmos_light):\n",
    "        # Unnormalize haze images\n",
    "        hazes_unnormalized = self.unnormalize_fun(x)\n",
    "        # Reconstruct clean images\n",
    "        nonhaze_rec = (hazes_unnormalized-atmos_light*(1-trans_map))\n",
    "        # nonhaze_rec = torch.clamp(nonhaze_rec, 0.0, 1.0)\n",
    "        nonhaze_rec = nonhaze_rec/trans_map\n",
    "        nonhaze_rec = torch.clamp(nonhaze_rec, 0.0, 1.0)\n",
    "        # Refinement Module\n",
    "        dehaze = self.generate_dehaze(nonhaze_rec, hazes_unnormalized, trans_map, atmos_light)\n",
    "        \n",
    "        return nonhaze_rec, dehaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['./result0728_exp3_real_haze/full_pass1x1/trans/aerial_trans.png', './result0728_exp3_real_haze/full_pass1x1/trans/castle_trans.png', './result0728_exp3_real_haze/full_pass1x1/trans/cityscape_trans.png']\n",
      "17\n",
      "['./result0728_exp3_real_haze/full_pass3x3/atmos_linear_blur_gaussian/aerial_atmos_blur.png', './result0728_exp3_real_haze/full_pass3x3/atmos_linear_blur_gaussian/castle_atmos_blur.png', './result0728_exp3_real_haze/full_pass3x3/atmos_linear_blur_gaussian/cityscape_atmos_blur.png']\n"
     ]
    }
   ],
   "source": [
    "# read tans and atmos images\n",
    "trans_dir = './result0728_exp3_real_haze/full_pass1x1/trans/'\n",
    "trans_name = []\n",
    "for root, dirs, files in os.walk(trans_dir):\n",
    "    for file in files:\n",
    "#         print(file)\n",
    "        trans_name.append(join(trans_dir,file))\n",
    "\n",
    "print(len(trans_name))\n",
    "print(trans_name[:3])\n",
    "\n",
    "atmos_dir = './result0728_exp3_real_haze/full_pass3x3/atmos_linear_blur_gaussian/'\n",
    "atmos_name = []\n",
    "for root, dirs, files in os.walk(atmos_dir):\n",
    "    for file in files:\n",
    "#         print(file)\n",
    "        atmos_name.append(join(atmos_dir,file))\n",
    "\n",
    "print(len(atmos_name))\n",
    "print(atmos_name[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Images Number: 17\n"
     ]
    }
   ],
   "source": [
    "test_loader, test_num = getTestLoader_Dehaze( './real_haze',1)\n",
    "print(\"Testing Images Number:\", test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model_dehaze(\n",
       "  (generate_dehaze): refinement_final(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(6, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (bn1): BatchNorm2d(64, eps=1.1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (dense_block1): BottleneckBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (residual_block11): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (residual_block12): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (dense_block2): BottleneckBlock(\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(48, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (residual_block21): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (residual_block22): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (get_dehaze_img): Sequential(\n",
       "      (0): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(\"device:\", device)\n",
    "checkpoint = torch.load('./exp3_epoch92_separate.pth')\n",
    "model_dehaze = Model_dehaze()\n",
    "model_dehaze = model_dehaze.to(device)\n",
    "model_dehaze.generate_dehaze.load_state_dict(checkpoint['dehaze'])\n",
    "model_dehaze.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: aerial\n",
      "original haze shape: torch.Size([1, 3, 442, 622])\n",
      "trans_map shape: torch.Size([1, 3, 442, 622])\n",
      "atmos_light shape: torch.Size([1, 3, 442, 622])\n",
      "Image: castle\n",
      "original haze shape: torch.Size([1, 3, 611, 619])\n",
      "trans_map shape: torch.Size([1, 3, 611, 619])\n",
      "atmos_light shape: torch.Size([1, 3, 611, 619])\n",
      "Image: cityscape\n",
      "original haze shape: torch.Size([1, 3, 600, 400])\n",
      "trans_map shape: torch.Size([1, 3, 600, 400])\n",
      "atmos_light shape: torch.Size([1, 3, 600, 400])\n",
      "Image: cliff\n",
      "original haze shape: torch.Size([1, 3, 384, 576])\n",
      "trans_map shape: torch.Size([1, 3, 384, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 384, 576])\n",
      "Image: forest\n",
      "original haze shape: torch.Size([1, 3, 768, 1024])\n",
      "trans_map shape: torch.Size([1, 3, 768, 1024])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 1024])\n",
      "Image: highquality13\n",
      "original haze shape: torch.Size([1, 3, 576, 768])\n",
      "trans_map shape: torch.Size([1, 3, 576, 768])\n",
      "atmos_light shape: torch.Size([1, 3, 576, 768])\n",
      "Image: img33\n",
      "original haze shape: torch.Size([1, 3, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 576])\n",
      "Image: img54\n",
      "original haze shape: torch.Size([1, 3, 763, 1180])\n",
      "trans_map shape: torch.Size([1, 3, 763, 1180])\n",
      "atmos_light shape: torch.Size([1, 3, 763, 1180])\n",
      "Image: img69\n",
      "original haze shape: torch.Size([1, 3, 1328, 1999])\n",
      "trans_map shape: torch.Size([1, 3, 1328, 1999])\n",
      "atmos_light shape: torch.Size([1, 3, 1328, 1999])\n",
      "Image: landscape\n",
      "original haze shape: torch.Size([1, 3, 525, 600])\n",
      "trans_map shape: torch.Size([1, 3, 525, 600])\n",
      "atmos_light shape: torch.Size([1, 3, 525, 600])\n",
      "Image: lviv\n",
      "original haze shape: torch.Size([1, 3, 1044, 1428])\n",
      "trans_map shape: torch.Size([1, 3, 1044, 1428])\n",
      "atmos_light shape: torch.Size([1, 3, 1044, 1428])\n",
      "Image: manhattan1\n",
      "original haze shape: torch.Size([1, 3, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 576])\n",
      "Image: manhattan2\n",
      "original haze shape: torch.Size([1, 3, 768, 1024])\n",
      "trans_map shape: torch.Size([1, 3, 768, 1024])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 1024])\n",
      "Image: redbrickshouse\n",
      "original haze shape: torch.Size([1, 3, 450, 441])\n",
      "trans_map shape: torch.Size([1, 3, 450, 441])\n",
      "atmos_light shape: torch.Size([1, 3, 450, 441])\n",
      "Image: road\n",
      "original haze shape: torch.Size([1, 3, 400, 600])\n",
      "trans_map shape: torch.Size([1, 3, 400, 600])\n",
      "atmos_light shape: torch.Size([1, 3, 400, 600])\n",
      "Image: swans\n",
      "original haze shape: torch.Size([1, 3, 416, 624])\n",
      "trans_map shape: torch.Size([1, 3, 416, 624])\n",
      "atmos_light shape: torch.Size([1, 3, 416, 624])\n",
      "Image: yosemite1\n",
      "original haze shape: torch.Size([1, 3, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 576])\n"
     ]
    }
   ],
   "source": [
    "img_dir= join('./result0728_exp3_real_haze','global_trans_local_atmos_blur')\n",
    "nonhaze_rec_list = []\n",
    "dehaze_list = []\n",
    "trans_map_list=[]\n",
    "atmos_map_list=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "        for i, test_batched in enumerate(test_loader,0):\n",
    "            print(\"Image:\",img_list[i])\n",
    "            # haze: (b,c,h,w) with (r,g,b) order\n",
    "            haze = test_batched['haze']\n",
    "            h_origin,w_origin,h_pad,w_pad = test_batched['info']\n",
    "            h_origin,w_origin,h_pad,w_pad = h_origin.item(),w_origin.item(),h_pad.item(),w_pad.item()\n",
    "#             print(\"reflected haze shape:\", haze.shape)\n",
    "#             print(\"haze info:\",h_origin,w_origin,h_pad,w_pad)\n",
    "            haze = haze[:,:,h_pad:h_pad+h_origin,w_pad:w_pad+w_origin]\n",
    "            print(\"original haze shape:\", haze.shape)\n",
    "            # trans_map: (b,c,h,w) with (r,g,b) order\n",
    "            trans_map = cv2.imread(trans_name[i])[:,:,::-1]\n",
    "            trans_map = torch.from_numpy(trans_map/255.).float().unsqueeze(0)\n",
    "            trans_map = trans_map.permute(0, 3, 1, 2)\n",
    "            print(\"trans_map shape:\", trans_map.shape)\n",
    "#             trans_map = trans_map.to(device)\n",
    "                      \n",
    "            atmos_light = cv2.imread(atmos_name[i])[:,:,::-1]\n",
    "            atmos_light = torch.from_numpy(atmos_light/255.).float().unsqueeze(0)\n",
    "            atmos_light = atmos_light.permute(0, 3, 1, 2)\n",
    "            print(\"atmos_light shape:\", atmos_light.shape)\n",
    "#             atmos_light = atmos_light.to(device)\n",
    "            \n",
    "#             print(\"haze datatype:\", haze.dtype)\n",
    "#             print(\"trans_map datatype:\", trans_map.dtype)\n",
    "#             print(\"atmos_light datatype:\", atmos_light.dtype)\n",
    "            \n",
    "            # Predict\n",
    "            nonhaze_rec, dehaze = model_dehaze(haze.to(device), trans_map.to(device), atmos_light.to(device))\n",
    "\n",
    "            for j in range(dehaze.size(0)):\n",
    "                # (C, H, W)\n",
    "                # nonhaze_rec_img = nonhaze_rec[j].detach().cpu().clone().numpy()\n",
    "                nonhaze_rec_img = nonhaze_rec[j].detach().cpu().clone().numpy()\n",
    "                dehaze_rec_img = dehaze[j].detach().cpu().clone().numpy()\n",
    "                trans_map_img = trans_map[j].detach().cpu().clone().numpy()\n",
    "                atmos_map_img = atmos_light[j].detach().cpu().clone().numpy()\n",
    "\n",
    "                # (H, W, C)\n",
    "                nonhaze_rec_img = (255*np.transpose(nonhaze_rec_img, (1,2,0))).astype(np.uint8)\n",
    "                dehaze_rec_img = (255*np.transpose(dehaze_rec_img, (1,2,0))).astype(np.uint8)\n",
    "                trans_map_img = (255*np.transpose(trans_map_img, (1,2,0))).astype(np.uint8)\n",
    "                atmos_map_img = (255*np.transpose(atmos_map_img, (1,2,0))).astype(np.uint8)\n",
    "                \n",
    "                # add for saving image results\n",
    "                nonhaze_rec_list.append(nonhaze_rec_img)\n",
    "                dehaze_list.append(dehaze_rec_img)\n",
    "                trans_map_list.append(trans_map_img)\n",
    "                atmos_map_list.append(atmos_map_img)\n",
    "                \n",
    "#                 cv2.imshow('Nonhaze Rec', nonhaze_rec_img[:,:,::-1])\n",
    "#                 cv2.imshow('Dehaze Result', dehaze_rec_img[:,:,::-1])\n",
    "#                 cv2.waitKey(0)\n",
    "#                 cv2.destroyAllWindows()\n",
    "            \n",
    "            atmos_intensity_list=[]\n",
    "            for idx in range(len(atmos_map_list)):\n",
    "                atmos_intensity_list.append(cv2.cvtColor(atmos_map_list[idx], cv2.COLOR_RGB2GRAY))\n",
    "\n",
    "            # because opencv is BGR order, we need to change RGB to BGR\n",
    "            for i in range(len(nonhaze_rec_list)):\n",
    "                cv2.imwrite(join(img_dir,'nonhaze_rec',img_list[i]+'_nonhaze_rec.png'),nonhaze_rec_list[i][:,:,::-1])\n",
    "            for i in range(len(dehaze_list)):\n",
    "                cv2.imwrite(join(img_dir,'dehaze',img_list[i]+'_dehaze.png'),dehaze_list[i][:,:,::-1])\n",
    "            for i in range(len(trans_map_list)):\n",
    "                cv2.imwrite(join(img_dir,'trans',img_list[i]+'_trans.png'),trans_map_list[i][:,:,::-1])\n",
    "            for i in range(len(atmos_map_list)):\n",
    "                cv2.imwrite(join(img_dir,'atmos',img_list[i]+'_atmos.png'),atmos_map_list[i][:,:,::-1])\n",
    "            for i in range(len(atmos_intensity_list)):\n",
    "                cv2.imwrite(join(img_dir,'atmos_intensity',img_list[i] +'_atmos_intensity.png'),atmos_intensity_list[i])\n",
    "#             for i in range(len(gt_list)):\n",
    "#                 cv2.imwrite(join(img_dir,'gt','gt'+str(i+1)+'.png'),gt_list[i][:,:,::-1])\n",
    "            \n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local trans + global atmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['./result0728_exp3_real_haze/full_pass3x3/trans_blur_final/aerial_trans_blur.png', './result0728_exp3_real_haze/full_pass3x3/trans_blur_final/castle_trans_blur.png', './result0728_exp3_real_haze/full_pass3x3/trans_blur_final/cityscape_trans_blur.png']\n",
      "17\n",
      "['./result0728_exp3_real_haze/full_pass1x1/atmos/aerial_atmos.png', './result0728_exp3_real_haze/full_pass1x1/atmos/castle_atmos.png', './result0728_exp3_real_haze/full_pass1x1/atmos/cityscape_atmos.png']\n"
     ]
    }
   ],
   "source": [
    "# read tans and atmos images\n",
    "trans_dir = './result0728_exp3_real_haze/full_pass3x3/trans_blur_final/'\n",
    "trans_name = []\n",
    "for root, dirs, files in os.walk(trans_dir):\n",
    "    for file in files:\n",
    "#         print(file)\n",
    "        trans_name.append(join(trans_dir,file))\n",
    "\n",
    "print(len(trans_name))\n",
    "print(trans_name[:3])\n",
    "\n",
    "atmos_dir = './result0728_exp3_real_haze/full_pass1x1/atmos/'\n",
    "atmos_name = []\n",
    "for root, dirs, files in os.walk(atmos_dir):\n",
    "    for file in files:\n",
    "#         print(file)\n",
    "        atmos_name.append(join(atmos_dir,file))\n",
    "\n",
    "print(len(atmos_name))\n",
    "print(atmos_name[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: aerial\n",
      "original haze shape: torch.Size([1, 3, 442, 622])\n",
      "trans_map shape: torch.Size([1, 3, 442, 622])\n",
      "atmos_light shape: torch.Size([1, 3, 442, 622])\n",
      "Image: castle\n",
      "original haze shape: torch.Size([1, 3, 611, 619])\n",
      "trans_map shape: torch.Size([1, 3, 611, 619])\n",
      "atmos_light shape: torch.Size([1, 3, 611, 619])\n",
      "Image: cityscape\n",
      "original haze shape: torch.Size([1, 3, 600, 400])\n",
      "trans_map shape: torch.Size([1, 3, 600, 400])\n",
      "atmos_light shape: torch.Size([1, 3, 600, 400])\n",
      "Image: cliff\n",
      "original haze shape: torch.Size([1, 3, 384, 576])\n",
      "trans_map shape: torch.Size([1, 3, 384, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 384, 576])\n",
      "Image: forest\n",
      "original haze shape: torch.Size([1, 3, 768, 1024])\n",
      "trans_map shape: torch.Size([1, 3, 768, 1024])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 1024])\n",
      "Image: highquality13\n",
      "original haze shape: torch.Size([1, 3, 576, 768])\n",
      "trans_map shape: torch.Size([1, 3, 576, 768])\n",
      "atmos_light shape: torch.Size([1, 3, 576, 768])\n",
      "Image: img33\n",
      "original haze shape: torch.Size([1, 3, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 576])\n",
      "Image: img54\n",
      "original haze shape: torch.Size([1, 3, 763, 1180])\n",
      "trans_map shape: torch.Size([1, 3, 763, 1180])\n",
      "atmos_light shape: torch.Size([1, 3, 763, 1180])\n",
      "Image: img69\n",
      "original haze shape: torch.Size([1, 3, 1328, 1999])\n",
      "trans_map shape: torch.Size([1, 3, 1328, 1999])\n",
      "atmos_light shape: torch.Size([1, 3, 1328, 1999])\n",
      "Image: landscape\n",
      "original haze shape: torch.Size([1, 3, 525, 600])\n",
      "trans_map shape: torch.Size([1, 3, 525, 600])\n",
      "atmos_light shape: torch.Size([1, 3, 525, 600])\n",
      "Image: lviv\n",
      "original haze shape: torch.Size([1, 3, 1044, 1428])\n",
      "trans_map shape: torch.Size([1, 3, 1044, 1428])\n",
      "atmos_light shape: torch.Size([1, 3, 1044, 1428])\n",
      "Image: manhattan1\n",
      "original haze shape: torch.Size([1, 3, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 576])\n",
      "Image: manhattan2\n",
      "original haze shape: torch.Size([1, 3, 768, 1024])\n",
      "trans_map shape: torch.Size([1, 3, 768, 1024])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 1024])\n",
      "Image: redbrickshouse\n",
      "original haze shape: torch.Size([1, 3, 450, 441])\n",
      "trans_map shape: torch.Size([1, 3, 450, 441])\n",
      "atmos_light shape: torch.Size([1, 3, 450, 441])\n",
      "Image: road\n",
      "original haze shape: torch.Size([1, 3, 400, 600])\n",
      "trans_map shape: torch.Size([1, 3, 400, 600])\n",
      "atmos_light shape: torch.Size([1, 3, 400, 600])\n",
      "Image: swans\n",
      "original haze shape: torch.Size([1, 3, 416, 624])\n",
      "trans_map shape: torch.Size([1, 3, 416, 624])\n",
      "atmos_light shape: torch.Size([1, 3, 416, 624])\n",
      "Image: yosemite1\n",
      "original haze shape: torch.Size([1, 3, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 576])\n"
     ]
    }
   ],
   "source": [
    "img_dir= join('./result0728_exp3_real_haze','local_trans_global_atmos')\n",
    "nonhaze_rec_list = []\n",
    "dehaze_list = []\n",
    "trans_map_list=[]\n",
    "atmos_map_list=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "        for i, test_batched in enumerate(test_loader,0):\n",
    "            print(\"Image:\",img_list[i])\n",
    "            # haze: (b,c,h,w) with (r,g,b) order\n",
    "            haze = test_batched['haze']\n",
    "            h_origin,w_origin,h_pad,w_pad = test_batched['info']\n",
    "            h_origin,w_origin,h_pad,w_pad = h_origin.item(),w_origin.item(),h_pad.item(),w_pad.item()\n",
    "#             print(\"reflected haze shape:\", haze.shape)\n",
    "#             print(\"haze info:\",h_origin,w_origin,h_pad,w_pad)\n",
    "            haze = haze[:,:,h_pad:h_pad+h_origin,w_pad:w_pad+w_origin]\n",
    "            print(\"original haze shape:\", haze.shape)\n",
    "            # trans_map: (b,c,h,w) with (r,g,b) order\n",
    "            trans_map = cv2.imread(trans_name[i])[:,:,::-1]\n",
    "            trans_map = torch.from_numpy(trans_map/255.).float().unsqueeze(0)\n",
    "            trans_map = trans_map.permute(0, 3, 1, 2)\n",
    "            print(\"trans_map shape:\", trans_map.shape)\n",
    "#             trans_map = trans_map.to(device)\n",
    "                      \n",
    "            atmos_light = cv2.imread(atmos_name[i])[:,:,::-1]\n",
    "            atmos_light = torch.from_numpy(atmos_light/255.).float().unsqueeze(0)\n",
    "            atmos_light = atmos_light.permute(0, 3, 1, 2)\n",
    "            print(\"atmos_light shape:\", atmos_light.shape)\n",
    "#             atmos_light = atmos_light.to(device)\n",
    "            \n",
    "#             print(\"haze datatype:\", haze.dtype)\n",
    "#             print(\"trans_map datatype:\", trans_map.dtype)\n",
    "#             print(\"atmos_light datatype:\", atmos_light.dtype)\n",
    "            \n",
    "            # Predict\n",
    "            nonhaze_rec, dehaze = model_dehaze(haze.to(device), trans_map.to(device), atmos_light.to(device))\n",
    "\n",
    "            for j in range(dehaze.size(0)):\n",
    "                # (C, H, W)\n",
    "                # nonhaze_rec_img = nonhaze_rec[j].detach().cpu().clone().numpy()\n",
    "                nonhaze_rec_img = nonhaze_rec[j].detach().cpu().clone().numpy()\n",
    "                dehaze_rec_img = dehaze[j].detach().cpu().clone().numpy()\n",
    "                trans_map_img = trans_map[j].detach().cpu().clone().numpy()\n",
    "                atmos_map_img = atmos_light[j].detach().cpu().clone().numpy()\n",
    "\n",
    "                # (H, W, C)\n",
    "                nonhaze_rec_img = (255*np.transpose(nonhaze_rec_img, (1,2,0))).astype(np.uint8)\n",
    "                dehaze_rec_img = (255*np.transpose(dehaze_rec_img, (1,2,0))).astype(np.uint8)\n",
    "                trans_map_img = (255*np.transpose(trans_map_img, (1,2,0))).astype(np.uint8)\n",
    "                atmos_map_img = (255*np.transpose(atmos_map_img, (1,2,0))).astype(np.uint8)\n",
    "                \n",
    "                # add for saving image results\n",
    "                nonhaze_rec_list.append(nonhaze_rec_img)\n",
    "                dehaze_list.append(dehaze_rec_img)\n",
    "                trans_map_list.append(trans_map_img)\n",
    "                atmos_map_list.append(atmos_map_img)\n",
    "                \n",
    "#                 cv2.imshow('Nonhaze Rec', nonhaze_rec_img[:,:,::-1])\n",
    "#                 cv2.imshow('Dehaze Result', dehaze_rec_img[:,:,::-1])\n",
    "#                 cv2.waitKey(0)\n",
    "#                 cv2.destroyAllWindows()\n",
    "#                 break\n",
    "            \n",
    "            atmos_intensity_list=[]\n",
    "            for idx in range(len(atmos_map_list)):\n",
    "                atmos_intensity_list.append(cv2.cvtColor(atmos_map_list[idx], cv2.COLOR_RGB2GRAY))\n",
    "\n",
    "            # because opencv is BGR order, we need to change RGB to BGR\n",
    "            for i in range(len(nonhaze_rec_list)):\n",
    "                cv2.imwrite(join(img_dir,'nonhaze_rec',img_list[i]+'_nonhaze_rec.png'),nonhaze_rec_list[i][:,:,::-1])\n",
    "            for i in range(len(dehaze_list)):\n",
    "                cv2.imwrite(join(img_dir,'dehaze',img_list[i]+'_dehaze.png'),dehaze_list[i][:,:,::-1])\n",
    "            for i in range(len(trans_map_list)):\n",
    "                cv2.imwrite(join(img_dir,'trans',img_list[i]+'_trans.png'),trans_map_list[i][:,:,::-1])\n",
    "            for i in range(len(atmos_map_list)):\n",
    "                cv2.imwrite(join(img_dir,'atmos',img_list[i]+'_atmos.png'),atmos_map_list[i][:,:,::-1])\n",
    "            for i in range(len(atmos_intensity_list)):\n",
    "                cv2.imwrite(join(img_dir,'atmos_intensity',img_list[i] +'_atmos_intensity.png'),atmos_intensity_list[i])\n",
    "#             for i in range(len(gt_list)):\n",
    "#                 cv2.imwrite(join(img_dir,'gt','gt'+str(i+1)+'.png'),gt_list[i][:,:,::-1])\n",
    "            \n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local trans + local atmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['./result0728_exp3_real_haze/full_pass3x3/trans_blur_final/aerial_trans_blur.png', './result0728_exp3_real_haze/full_pass3x3/trans_blur_final/castle_trans_blur.png', './result0728_exp3_real_haze/full_pass3x3/trans_blur_final/cityscape_trans_blur.png']\n",
      "17\n",
      "['./result0728_exp3_real_haze/full_pass3x3/atmos_linear_blur_gaussian/aerial_atmos_blur.png', './result0728_exp3_real_haze/full_pass3x3/atmos_linear_blur_gaussian/castle_atmos_blur.png', './result0728_exp3_real_haze/full_pass3x3/atmos_linear_blur_gaussian/cityscape_atmos_blur.png']\n"
     ]
    }
   ],
   "source": [
    "# read tans and atmos images\n",
    "trans_dir = './result0728_exp3_real_haze/full_pass3x3/trans_blur_final/'\n",
    "trans_name = []\n",
    "for root, dirs, files in os.walk(trans_dir):\n",
    "    for file in files:\n",
    "#         print(file)\n",
    "        trans_name.append(join(trans_dir,file))\n",
    "\n",
    "print(len(trans_name))\n",
    "print(trans_name[:3])\n",
    "\n",
    "atmos_dir = './result0728_exp3_real_haze/full_pass3x3/atmos_linear_blur_gaussian/'\n",
    "atmos_name = []\n",
    "for root, dirs, files in os.walk(atmos_dir):\n",
    "    for file in files:\n",
    "#         print(file)\n",
    "        atmos_name.append(join(atmos_dir,file))\n",
    "\n",
    "print(len(atmos_name))\n",
    "print(atmos_name[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: aerial\n",
      "original haze shape: torch.Size([1, 3, 442, 622])\n",
      "trans_map shape: torch.Size([1, 3, 442, 622])\n",
      "atmos_light shape: torch.Size([1, 3, 442, 622])\n",
      "Image: castle\n",
      "original haze shape: torch.Size([1, 3, 611, 619])\n",
      "trans_map shape: torch.Size([1, 3, 611, 619])\n",
      "atmos_light shape: torch.Size([1, 3, 611, 619])\n",
      "Image: cityscape\n",
      "original haze shape: torch.Size([1, 3, 600, 400])\n",
      "trans_map shape: torch.Size([1, 3, 600, 400])\n",
      "atmos_light shape: torch.Size([1, 3, 600, 400])\n",
      "Image: cliff\n",
      "original haze shape: torch.Size([1, 3, 384, 576])\n",
      "trans_map shape: torch.Size([1, 3, 384, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 384, 576])\n",
      "Image: forest\n",
      "original haze shape: torch.Size([1, 3, 768, 1024])\n",
      "trans_map shape: torch.Size([1, 3, 768, 1024])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 1024])\n",
      "Image: highquality13\n",
      "original haze shape: torch.Size([1, 3, 576, 768])\n",
      "trans_map shape: torch.Size([1, 3, 576, 768])\n",
      "atmos_light shape: torch.Size([1, 3, 576, 768])\n",
      "Image: img33\n",
      "original haze shape: torch.Size([1, 3, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 576])\n",
      "Image: img54\n",
      "original haze shape: torch.Size([1, 3, 763, 1180])\n",
      "trans_map shape: torch.Size([1, 3, 763, 1180])\n",
      "atmos_light shape: torch.Size([1, 3, 763, 1180])\n",
      "Image: img69\n",
      "original haze shape: torch.Size([1, 3, 1328, 1999])\n",
      "trans_map shape: torch.Size([1, 3, 1328, 1999])\n",
      "atmos_light shape: torch.Size([1, 3, 1328, 1999])\n",
      "Image: landscape\n",
      "original haze shape: torch.Size([1, 3, 525, 600])\n",
      "trans_map shape: torch.Size([1, 3, 525, 600])\n",
      "atmos_light shape: torch.Size([1, 3, 525, 600])\n",
      "Image: lviv\n",
      "original haze shape: torch.Size([1, 3, 1044, 1428])\n",
      "trans_map shape: torch.Size([1, 3, 1044, 1428])\n",
      "atmos_light shape: torch.Size([1, 3, 1044, 1428])\n",
      "Image: manhattan1\n",
      "original haze shape: torch.Size([1, 3, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 576])\n",
      "Image: manhattan2\n",
      "original haze shape: torch.Size([1, 3, 768, 1024])\n",
      "trans_map shape: torch.Size([1, 3, 768, 1024])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 1024])\n",
      "Image: redbrickshouse\n",
      "original haze shape: torch.Size([1, 3, 450, 441])\n",
      "trans_map shape: torch.Size([1, 3, 450, 441])\n",
      "atmos_light shape: torch.Size([1, 3, 450, 441])\n",
      "Image: road\n",
      "original haze shape: torch.Size([1, 3, 400, 600])\n",
      "trans_map shape: torch.Size([1, 3, 400, 600])\n",
      "atmos_light shape: torch.Size([1, 3, 400, 600])\n",
      "Image: swans\n",
      "original haze shape: torch.Size([1, 3, 416, 624])\n",
      "trans_map shape: torch.Size([1, 3, 416, 624])\n",
      "atmos_light shape: torch.Size([1, 3, 416, 624])\n",
      "Image: yosemite1\n",
      "original haze shape: torch.Size([1, 3, 768, 576])\n",
      "trans_map shape: torch.Size([1, 3, 768, 576])\n",
      "atmos_light shape: torch.Size([1, 3, 768, 576])\n"
     ]
    }
   ],
   "source": [
    "img_dir= join('./result0728_exp3_real_haze','local_trans_local_atmos')\n",
    "nonhaze_rec_list = []\n",
    "dehaze_list = []\n",
    "trans_map_list=[]\n",
    "atmos_map_list=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "        for i, test_batched in enumerate(test_loader,0):\n",
    "            print(\"Image:\",img_list[i])\n",
    "            # haze: (b,c,h,w) with (r,g,b) order\n",
    "            haze = test_batched['haze']\n",
    "            h_origin,w_origin,h_pad,w_pad = test_batched['info']\n",
    "            h_origin,w_origin,h_pad,w_pad = h_origin.item(),w_origin.item(),h_pad.item(),w_pad.item()\n",
    "#             print(\"reflected haze shape:\", haze.shape)\n",
    "#             print(\"haze info:\",h_origin,w_origin,h_pad,w_pad)\n",
    "            haze = haze[:,:,h_pad:h_pad+h_origin,w_pad:w_pad+w_origin]\n",
    "            print(\"original haze shape:\", haze.shape)\n",
    "            # trans_map: (b,c,h,w) with (r,g,b) order\n",
    "            trans_map = cv2.imread(trans_name[i])[:,:,::-1]\n",
    "            trans_map = torch.from_numpy(trans_map/255.).float().unsqueeze(0)\n",
    "            trans_map = trans_map.permute(0, 3, 1, 2)\n",
    "            print(\"trans_map shape:\", trans_map.shape)\n",
    "#             trans_map = trans_map.to(device)\n",
    "                      \n",
    "            atmos_light = cv2.imread(atmos_name[i])[:,:,::-1]\n",
    "            atmos_light = torch.from_numpy(atmos_light/255.).float().unsqueeze(0)\n",
    "            atmos_light = atmos_light.permute(0, 3, 1, 2)\n",
    "            print(\"atmos_light shape:\", atmos_light.shape)\n",
    "#             atmos_light = atmos_light.to(device)\n",
    "            \n",
    "#             print(\"haze datatype:\", haze.dtype)\n",
    "#             print(\"trans_map datatype:\", trans_map.dtype)\n",
    "#             print(\"atmos_light datatype:\", atmos_light.dtype)\n",
    "            \n",
    "            # Predict\n",
    "            nonhaze_rec, dehaze = model_dehaze(haze.to(device), trans_map.to(device), atmos_light.to(device))\n",
    "\n",
    "            for j in range(dehaze.size(0)):\n",
    "                # (C, H, W)\n",
    "                # nonhaze_rec_img = nonhaze_rec[j].detach().cpu().clone().numpy()\n",
    "                nonhaze_rec_img = nonhaze_rec[j].detach().cpu().clone().numpy()\n",
    "                dehaze_rec_img = dehaze[j].detach().cpu().clone().numpy()\n",
    "                trans_map_img = trans_map[j].detach().cpu().clone().numpy()\n",
    "                atmos_map_img = atmos_light[j].detach().cpu().clone().numpy()\n",
    "\n",
    "                # (H, W, C)\n",
    "                nonhaze_rec_img = (255*np.transpose(nonhaze_rec_img, (1,2,0))).astype(np.uint8)\n",
    "                dehaze_rec_img = (255*np.transpose(dehaze_rec_img, (1,2,0))).astype(np.uint8)\n",
    "                trans_map_img = (255*np.transpose(trans_map_img, (1,2,0))).astype(np.uint8)\n",
    "                atmos_map_img = (255*np.transpose(atmos_map_img, (1,2,0))).astype(np.uint8)\n",
    "                \n",
    "                # add for saving image results\n",
    "                nonhaze_rec_list.append(nonhaze_rec_img)\n",
    "                dehaze_list.append(dehaze_rec_img)\n",
    "                trans_map_list.append(trans_map_img)\n",
    "                atmos_map_list.append(atmos_map_img)\n",
    "                \n",
    "#                 cv2.imshow('Nonhaze Rec', nonhaze_rec_img[:,:,::-1])\n",
    "#                 cv2.imshow('Dehaze Result', dehaze_rec_img[:,:,::-1])\n",
    "#                 cv2.waitKey(0)\n",
    "#                 cv2.destroyAllWindows()\n",
    "#                 break\n",
    "            \n",
    "            atmos_intensity_list=[]\n",
    "            for idx in range(len(atmos_map_list)):\n",
    "                atmos_intensity_list.append(cv2.cvtColor(atmos_map_list[idx], cv2.COLOR_RGB2GRAY))\n",
    "\n",
    "            # because opencv is BGR order, we need to change RGB to BGR\n",
    "            for i in range(len(nonhaze_rec_list)):\n",
    "                cv2.imwrite(join(img_dir,'nonhaze_rec',img_list[i]+'_nonhaze_rec.png'),nonhaze_rec_list[i][:,:,::-1])\n",
    "            for i in range(len(dehaze_list)):\n",
    "                cv2.imwrite(join(img_dir,'dehaze',img_list[i]+'_dehaze.png'),dehaze_list[i][:,:,::-1])\n",
    "            for i in range(len(trans_map_list)):\n",
    "                cv2.imwrite(join(img_dir,'trans',img_list[i]+'_trans.png'),trans_map_list[i][:,:,::-1])\n",
    "            for i in range(len(atmos_map_list)):\n",
    "                cv2.imwrite(join(img_dir,'atmos',img_list[i]+'_atmos.png'),atmos_map_list[i][:,:,::-1])\n",
    "            for i in range(len(atmos_intensity_list)):\n",
    "                cv2.imwrite(join(img_dir,'atmos_intensity',img_list[i] +'_atmos_intensity.png'),atmos_intensity_list[i])\n",
    "#             for i in range(len(gt_list)):\n",
    "#                 cv2.imwrite(join(img_dir,'gt','gt'+str(i+1)+'.png'),gt_list[i][:,:,::-1])\n",
    "            \n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_conda_gpu)",
   "language": "python",
   "name": "pytorch_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
